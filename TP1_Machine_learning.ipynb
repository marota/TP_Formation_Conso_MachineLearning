{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le problème\n",
    "\n",
    "Pour garantir l'équilibre offre-demande à chaque instant, RTE construit ses propres prévisions de la consommation nationale, régionale, et locale. \n",
    "\n",
    "Nous nous concentrons ici sur la prévision nationale. Un challenge lancé par RTE (https://dataanalyticspost.com/wp-content/uploads/2017/06/Challenge-RTE-Prevision-de-Consommation.pdf) a permis de tester des approches alternatives aux modèles internes (POPCORN, PREMIS).\n",
    "\n",
    "<img src=\"pictures/ChallengeConso.png\" width=1000 height=100>\n",
    "\n",
    "Comme dans ce challenge, nous voulons aider RTE a faire de meilleures prévisions de conso ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un outil: le Machine Learning\n",
    "\n",
    "Pour cela nous allons avoir recours au Machine Learning. Cela nous permettra de créer un modèle qui apprend et s'adapte au contexte sans programmer un système expert avec des \"centaines\" de règles en dur par de la programmation logique. \n",
    "\n",
    "Le Machine Learning nécessite toutefois de la connaissance experte dans le domaine d'intérêt pour créer des modèles pertinents et efficaces. En effet, si notre modèle embarque trop de variables peu explicatives, il sera noyé dans l'information, surapprendra sur les exemples qu'on lui a montrés, et aura du mal à généraliser en prédisant avec justesse sur de nouveaux exemples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une difficulté: le feature engineering\n",
    "\n",
    "Au-delà de la simple sélection de variables pertinentes, on fait surtout ce que l'on appelle du feature engineering avec notre expertise: on crée des variables transformées ou aggrégées, comme une consommation moyenne sur le mois précédent ou une température moyenne sur la France, pour guider l'algorithme à apprendre sur l'information la plus pertinente et synthétique. Cela implique de bien connaître nos données, de passer du temps à les visualiser, et de les prétraiter avant de les fournir au modèle de machine-learning.\n",
    "\n",
    "Nous allons ici voir ce que cela implique en terme de développement et d'implémentation de participer à un tel challenge, en montrant les capacités du Machine Learning sur la base de modèles \"classiques\".\n",
    "\n",
    "## Ce que l'on va voir dans ce premier TP :\n",
    "1) Formaliser le problème: que souhaite-t-on prédire (quel est mon Y) ? Avec quelles variables explicatives (quel est mon X) ?\n",
    "\n",
    "2) Collecter les données: où se trouvent les données ? Quel est le format ? Comment les récupérer ? (FACULTATIF - voir TP \"TP1_Preparation_donnees\")\n",
    "\n",
    "3) Investiguer les données: visualiser des séries temporelles, faire quelques statistiques descriptives\n",
    "\n",
    "4) Préparer les données: pour entrainer et tester un premier modèle\n",
    "\n",
    "5) Créer et entrainer un premier modèle simple: ce sera notre baseline\n",
    "\n",
    "6) Evaluer un modèle\n",
    "\n",
    "7) Itérer en créant de nouveaux modèles avec de nouvelles variables explicatives\n",
    "\n",
    "8) Jouez: créer vos propres modèles, tester sur une saison différente, tester sur une région différente, faire une prévision avec incertitudes, détecter des outliers\n",
    "\n",
    "## To be continued\n",
    "Le deuxième TP permettra d'investiguer les modèles \"Deep\" avec réseaux de neurones, en montrant le moindre besoin en feature engineering et leur plus grande capacité a absorber l'information de par les représentations hiérarchiques qu'ils se créent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionnement en temps\n",
    "On prévoit un une durée d'environ 2h pour ce TP1, debrief inclus :\n",
    "- 20-30 minutes pour charger et préparer les données [FACULTATIF]\n",
    "- 30-40 minutes pour analyser et visualiser les données\n",
    "- 45-60 minutes pour créer, entrainer, évaluer et interpréter les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Se familiariser avec le problème: Eco2mix\n",
    "Quand on parle de courbe de consommation France, il y a une application incontournable : eco2mix !\n",
    "Allons voir à quoi ressemblent ces courbes de consommation, pour nous faire une idée du problème et se donner quelques intuitions:\n",
    "http://www.rte-france.com/fr/eco2mix/eco2mix\n",
    "ou sur application mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On passe au code : import de librairies et configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbprophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c25806456aab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfbprophet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProphet\u001b[0m  \u001b[0;31m# un package de series temporelles mis a disposition par facebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m  \u001b[0;31m# move ou copier fichier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m  \u001b[0;31m# compresser ou décompresser fichier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbprophet'"
     ]
    }
   ],
   "source": [
    "# Exécutez la cellule ci-dessous (par exemple avec shift-entrée)\n",
    "# Si vous exécuter ce notebook depuis votre PC, il faudra peut-etre installer certaines librairies avec \n",
    "# 'pip install ma_librairie'\n",
    "import os  # accès aux commandes système\n",
    "import datetime  # structure de données pour gérer des objets calendaires\n",
    "import pandas as pd  # gérer des tables de données en python\n",
    "import numpy as np  # librairie d'opérations mathématiques\n",
    "import matplotlib.pyplot as plt  # tracer des visualisations\n",
    "import sklearn  # librairie de machine learning\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from fbprophet import Prophet  # un package de series temporelles mis a disposition par facebook\n",
    "import shutil  # move ou copier fichier\n",
    "import zipfile  # compresser ou décompresser fichier\n",
    "import urllib3 # téléchargement de fichier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Choix du répertoire de travail \"data_folder\" dans lequel tous les fichiers csv seront entreposés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(), \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon repertoire est : /home/marotant/dev/TP_Formation_Conso_MachineLearning/data\n",
      "Fichiers contenus dans ce répertoire :\n",
      " - Xinput.zip\n",
      " - joursFeries.csv\n",
      " - Xinput_old.csv\n",
      " - YconsoT0.csv\n",
      " - Xinput.csv\n",
      " - StationsMeteoRTE.csv\n",
      " - communes_coordonnees.csv\n",
      " - Yconso.csv\n",
      " - eCO2mix_RTE_tempo_2017-2018.xls\n",
      " - meteoX_T0_T24.zip\n"
     ]
    }
   ],
   "source": [
    "# Petite vérification\n",
    "print(\"Mon repertoire est : {}\".format(data_folder))\n",
    "print(\"Fichiers contenus dans ce répertoire :\")\n",
    "for file in os.listdir(data_folder):\n",
    "    print(\" - \" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération des données\n",
    "\n",
    "Dans cette partie nous allons charger les fichiers csv nécessaires pour l'analyse, puis les convertir en data-frame python.\n",
    "Les données brutes ont été pré-traitées à l'aide du notebook TP1_Preparation_donnees :\n",
    "- Yconso.csv\n",
    "- Xinput.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "Yconso_csv = os.path.join(data_folder, \"Yconso.csv\")\n",
    "Yconso = pd.read_csv(Yconso_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La colonne \"ds\" contient des objets de type string. On va la convertir en objets de type \"datetime\" plus approprié.  \n",
    "Pour plus d'information, voir le TP1_Preparation_donnees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yconso['ds'] = pd.to_datetime(Yconso['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   ds      y\n",
      "0 2012-12-28 00:00:00  59679\n",
      "1 2012-12-28 01:00:00  55354\n",
      "2 2012-12-28 02:00:00  54324\n",
      "3 2012-12-28 03:00:00  52066\n",
      "4 2012-12-28 04:00:00  49684\n",
      "(39049, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Yconso.head(5))\n",
    "print(Yconso.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention : Les données Xinput sont encryptées dans un fichier zip. du fait de données météo**  \n",
    "Pour les lire vous avez besoin d'un mot de passe qui ne peut vous être donné que dans le cadre d'un travail au sein de RTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput_zip = os.path.join(data_folder, \"Xinput.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour travailler avec les fichiers zip, on utilise la bibliothèque **zipfile**.\n",
    "zipfile_xinput = zipfile.ZipFile(Xinput_zip)\n",
    "zipfile_xinput.setpassword(bytes(password,'utf-8'))\n",
    "Xinput = pd.read_csv(zipfile_xinput.open('Xinput.csv'),sep=\",\",engine='c',header=0)\n",
    "\n",
    "Xinput['ds'] = pd.to_datetime(Xinput['ds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xconso_csv = os.path.join(data_folder, \"Xinput.csv\")\n",
    "Xinput = pd.read_csv(Xconso_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ds  X002Th+0  X005Th+0  X015Th+0  X027Th+0  X070Th+0  \\\n",
      "0   2012-12-28 00:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "1   2012-12-28 01:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "2   2012-12-28 02:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "3   2012-12-28 03:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "4   2012-12-28 04:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "5   2012-12-28 05:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "6   2012-12-28 06:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "7   2012-12-28 07:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "8   2012-12-28 08:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "9   2012-12-28 09:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "10  2012-12-28 10:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "11  2012-12-28 11:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "12  2012-12-28 12:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "13  2012-12-28 13:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "14  2012-12-28 14:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "15  2012-12-28 15:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "16  2012-12-28 16:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "17  2012-12-28 17:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "18  2012-12-28 18:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "19  2012-12-28 19:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "20  2012-12-28 20:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "21  2012-12-28 21:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "22  2012-12-28 22:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "23  2012-12-28 23:00:00       NaN       NaN       NaN       NaN       NaN   \n",
      "24  2012-12-29 00:00:00      9.50     11.90     10.50     11.70     10.10   \n",
      "25  2012-12-29 01:00:00      9.50     11.90     10.50     11.69     10.11   \n",
      "26  2012-12-29 02:00:00      9.50     11.89     10.51     11.68     10.12   \n",
      "27  2012-12-29 03:00:00      9.51     11.88     10.53     11.66     10.14   \n",
      "28  2012-12-29 04:00:00      9.51     11.86     10.54     11.63     10.17   \n",
      "29  2012-12-29 05:00:00      9.52     11.84     10.56     11.60     10.21   \n",
      "30  2012-12-29 06:00:00      9.52     11.82     10.59     11.56     10.25   \n",
      "31  2012-12-29 07:00:00      9.53     11.80     10.62     11.52     10.30   \n",
      "32  2012-12-29 08:00:00      9.54     11.77     10.65     11.47     10.35   \n",
      "33  2012-12-29 09:00:00      9.55     11.74     10.68     11.41     10.41   \n",
      "34  2012-12-29 10:00:00      9.56     11.70     10.72     11.35     10.48   \n",
      "\n",
      "    X110Th+0  X120Th+0  X130Th+0  X145Th+0     ...       X630Th+24  X643Th+24  \\\n",
      "0        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "1        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "2        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "3        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "4        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "5        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "6        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "7        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "8        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "9        NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "10       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "11       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "12       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "13       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "14       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "15       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "16       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "17       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "18       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "19       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "20       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "21       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "22       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "23       NaN       NaN       NaN       NaN     ...             NaN        NaN   \n",
      "24     12.40     13.00     11.80      9.90     ...            7.60       9.90   \n",
      "25     12.40     13.01     11.80      9.90     ...            7.60       9.90   \n",
      "26     12.39     13.02     11.81      9.89     ...            7.60       9.91   \n",
      "27     12.39     13.05     11.82      9.89     ...            7.61       9.91   \n",
      "28     12.38     13.08     11.83      9.88     ...            7.62       9.92   \n",
      "29     12.37     13.11     11.85      9.87     ...            7.62       9.93   \n",
      "30     12.36     13.16     11.87      9.86     ...            7.63       9.95   \n",
      "31     12.35     13.21     11.89      9.85     ...            7.64       9.96   \n",
      "32     12.34     13.27     11.92      9.84     ...            7.65       9.98   \n",
      "33     12.32     13.33     11.95      9.82     ...            7.67      10.00   \n",
      "34     12.31     13.40     11.98      9.80     ...            7.68      10.02   \n",
      "\n",
      "    X645Th+24  X650Th+24  X675Th+24  X690Th+24  X747Th+24  holiday  \\\n",
      "0         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "1         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "2         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "3         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "4         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "5         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "6         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "7         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "8         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "9         NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "10        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "11        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "12        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "13        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "14        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "15        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "16        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "17        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "18        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "19        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "20        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "21        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "22        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "23        NaN        NaN        NaN        NaN        NaN      NaN   \n",
      "24      11.10      11.60      11.80      11.10      12.00      NaN   \n",
      "25      11.10      11.60      11.80      11.10      12.01      NaN   \n",
      "26      11.11      11.61      11.81      11.10      12.02      NaN   \n",
      "27      11.11      11.61      11.81      11.11      12.04      NaN   \n",
      "28      11.12      11.62      11.82      11.11      12.06      NaN   \n",
      "29      11.13      11.64      11.84      11.12      12.09      NaN   \n",
      "30      11.13      11.65      11.85      11.12      12.12      NaN   \n",
      "31      11.15      11.66      11.86      11.13      12.16      NaN   \n",
      "32      11.16      11.68      11.88      11.14      12.21      NaN   \n",
      "33      11.17      11.70      11.90      11.15      12.26      NaN   \n",
      "34      11.19      11.72      11.92      11.15      12.31      NaN   \n",
      "\n",
      "    FranceTh+0  FranceTh+24  \n",
      "0          NaN          NaN  \n",
      "1          NaN          NaN  \n",
      "2          NaN          NaN  \n",
      "3          NaN          NaN  \n",
      "4          NaN          NaN  \n",
      "5          NaN          NaN  \n",
      "6          NaN          NaN  \n",
      "7          NaN          NaN  \n",
      "8          NaN          NaN  \n",
      "9          NaN          NaN  \n",
      "10         NaN          NaN  \n",
      "11         NaN          NaN  \n",
      "12         NaN          NaN  \n",
      "13         NaN          NaN  \n",
      "14         NaN          NaN  \n",
      "15         NaN          NaN  \n",
      "16         NaN          NaN  \n",
      "17         NaN          NaN  \n",
      "18         NaN          NaN  \n",
      "19         NaN          NaN  \n",
      "20         NaN          NaN  \n",
      "21         NaN          NaN  \n",
      "22         NaN          NaN  \n",
      "23         NaN          NaN  \n",
      "24    9.339850    10.734500  \n",
      "25    9.339580    10.735110  \n",
      "26    9.337955    10.733485  \n",
      "27    9.338565    10.733050  \n",
      "28    9.337475    10.731165  \n",
      "29    9.335850    10.730180  \n",
      "30    9.332855    10.728010  \n",
      "31    9.332975    10.725145  \n",
      "32    9.332275    10.721535  \n",
      "33    9.327870    10.717750  \n",
      "34    9.325125    10.714975  \n",
      "\n",
      "[35 rows x 74 columns]\n",
      "(39049, 74)\n",
      "Index(['ds', 'X002Th+0', 'X005Th+0', 'X015Th+0', 'X027Th+0', 'X070Th+0',\n",
      "       'X110Th+0', 'X120Th+0', 'X130Th+0', 'X145Th+0', 'X149Th+0', 'X156Th+0',\n",
      "       'X168Th+0', 'X180Th+0', 'X190Th+0', 'X222Th+0', 'X240Th+0', 'X255Th+0',\n",
      "       'X260Th+0', 'X280Th+0', 'X299Th+0', 'X434Th+0', 'X460Th+0', 'X481Th+0',\n",
      "       'X497Th+0', 'X510Th+0', 'X579Th+0', 'X588Th+0', 'X621Th+0', 'X630Th+0',\n",
      "       'X643Th+0', 'X645Th+0', 'X650Th+0', 'X675Th+0', 'X690Th+0', 'X747Th+0',\n",
      "       'X002Th+24', 'X005Th+24', 'X015Th+24', 'X027Th+24', 'X070Th+24',\n",
      "       'X110Th+24', 'X120Th+24', 'X130Th+24', 'X145Th+24', 'X149Th+24',\n",
      "       'X156Th+24', 'X168Th+24', 'X180Th+24', 'X190Th+24', 'X222Th+24',\n",
      "       'X240Th+24', 'X255Th+24', 'X260Th+24', 'X280Th+24', 'X299Th+24',\n",
      "       'X434Th+24', 'X460Th+24', 'X481Th+24', 'X497Th+24', 'X510Th+24',\n",
      "       'X579Th+24', 'X588Th+24', 'X621Th+24', 'X630Th+24', 'X643Th+24',\n",
      "       'X645Th+24', 'X650Th+24', 'X675Th+24', 'X690Th+24', 'X747Th+24',\n",
      "       'holiday', 'FranceTh+0', 'FranceTh+24'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(Xinput.head(35))\n",
    "print(Xinput.shape)\n",
    "print(Xinput.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des données \n",
    "\n",
    "La DataScience et le Machine Learning supposent de bien appréhender les données sur lesquelles nos modèles vont être entrainés. Pour se faire, il est utile de se faire quelques stats descriptives et des visualisations pour nos différentes variables.\n",
    "\n",
    "Traitant d'un problème de prévisions, on visualisera en particulier des séries temporelles.\n",
    "\n",
    "Vous allez voir:\n",
    "- échantillons de données\n",
    "- profils de courbe de consommation journaliers et saisonniers\n",
    "- visualisation de corrélation entre conso J et conso retardée\n",
    "- visualisations des stations météos\n",
    "- visualisations des séries temporelles des températures\n",
    "- calcul de corrélation sur la température entre les différentes stations météo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de statistiques descriptives sur la consommation nationale\n",
    "A l'aide de la fonction _describe_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yconso['y'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la consommation d'un jour particulier\n",
    "On souhaite visualiser la consommation réalisée pour un jour donné de l'historique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load(var_load, year, month, day):\n",
    "    date_cible = datetime.datetime(year=year, month=month, day=day)  # implicitement heure = minuit\n",
    "    date_lendemain_cible = date_cible + datetime.timedelta(days=1)\n",
    "    mask = (var_load.ds >= date_cible) & (var_load.ds <= date_lendemain_cible)   \n",
    "    consoJour = var_load[mask]\n",
    "    plt.plot(consoJour['ds'], consoJour['y'], color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_load(Yconso, 2016, 12, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afficher une semaine arbitraire de consommation\n",
    "On pourra modifier la fonction précédente en rajoutant le timedelta en paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load_timedelta(var_load, year, month, day, delta_days):\n",
    "    date_cible = datetime.datetime(year=year, month=month, day=day)\n",
    "    date_lendemain_cible = date_cible + datetime.timedelta(days=delta_days)\n",
    "\n",
    "    conso_periode = var_load[(var_load.ds >= date_cible) \n",
    "                                      & (var_load.ds <= date_lendemain_cible)]\n",
    "    plt.plot(conso_periode['ds'], conso_periode['y'], color='blue')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_load_timedelta(Yconso, 2016, 12, 20, delta_days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation des profils de la consommation pour les mois d'hiver et les mois d'été\n",
    "Toujours dans le but d'appréhender nos données, on va regarder les profils moyens pour le smois d'été et pour ceux d'hiver. On va également observer le min et le max pour avoir une idée de la variabilité du signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par commodité, on isole le mois pour après attraper les mois d'hiver et d'été\n",
    "Xinput['month'] = Xinput['ds'].dt.month\n",
    "\n",
    "# On isole aussi les heures\n",
    "Xinput['hour'] = Xinput['ds'].dt.hour\n",
    "\n",
    "# On sépare les jours de la semaine en week-end / pas week-end\n",
    "# De base, la fonction datetime.weekday() renvoie 0 => Lundi, 2 => Mardi, ..., 5 => Samedi, 6 => Dimanche\n",
    "# Ci-dessous, si on a un jour d ela semaine alors dans la colonne weekday on mettra 1, et 0 si c'est le week-end\n",
    "Xinput['weekday'] = (Xinput['ds'].dt.weekday < 5).astype(int)  # conversion bool => int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On aggrège les mois d'hiver ensemble\n",
    "XY_df = pd.merge(Yconso, Xinput, on = 'ds')\n",
    "groupedHiver = XY_df[(XY_df.month == 12) | \n",
    "                                     (XY_df.month == 1) | \n",
    "                                     (XY_df.month == 2)].groupby(['weekday', 'hour'], as_index=True)\n",
    "\n",
    "# Idem pour les mois d'été\n",
    "groupedEte = XY_df[(XY_df.month == 6) | \n",
    "                                   (XY_df.month == 7) | \n",
    "                                   (XY_df.month == 8)].groupby(['weekday', 'hour'], as_index=True)\n",
    "\n",
    "statsHiver = groupedHiver['y'].aggregate([np.mean, np.min, np.max])\n",
    "statsEte = groupedEte['y'].aggregate([np.mean, np.min, np.max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statsHiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche des infos sur le profil pour les jours de la semaine\n",
    "semaine = statsHiver.loc[1]  # 0 pour les jours de semaine\n",
    "weekend = statsHiver.loc[0]  # 0 pour weekend\n",
    "\n",
    "plt.plot(semaine['amin'], color='cyan')\n",
    "plt.plot(semaine['mean'], color='blue')\n",
    "plt.plot(semaine['amax'], color='cyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lien avec la consommation passée\n",
    "A l'aide de la fonction shift, pour un point horaire cible on regarde  :\n",
    "- la consommation de l'heure précédente, \n",
    "- du jour précédent, \n",
    "- de la semaine précédente.\n",
    "\n",
    "On regarde ensuite si la consommation réalisé peut se deviner à partir de ces observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput['lag1H'] = Yconso['y'].shift(1)\n",
    "Xinput['lag1D'] = Yconso['y'].shift(24)\n",
    "Xinput['lag1W'] = Yconso['y'].shift(24*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput.head(24 * 7 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On regarde maintenant graphiquement si on a une belle corrélation ou non :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_load(var_x):\n",
    "    plt.scatter(Xinput[var_x],Yconso['y'])\n",
    "    plt.title(var_x)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_load('lag1H')\n",
    "plot_scatter_load('lag1D')\n",
    "plot_scatter_load('lag1W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Que pensez-vous de ces corrélations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des stations météo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardons si les températures des stations météo sont corrélées entre elles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#matrix_correlation = meteo_obs_df.corr() #calcul d'une corrélation globale\n",
    "cols = list(Xinput.columns[Xinput.columns.str.endswith(\"Th_prev\")])\n",
    "#calcul de la corrélation en fonction de la saison\n",
    "\n",
    "Xinput['saison'] = ((Xinput['ds'].dt.month ==1) |(Xinput['ds'].dt.month==2)|(Xinput['ds'].dt.month==12)).astype(int)*1+((Xinput['ds'].dt.month ==3 )|(Xinput['ds'].dt.month==4)|(Xinput['ds'].dt.month==5)).astype(int)*2+((Xinput['ds'].dt.month ==6 )|(Xinput['ds'].dt.month==7)|(Xinput['ds'].dt.month==8)).astype(int)*3+((Xinput['ds'].dt.month ==9) |(Xinput['ds'].dt.month==10)|(Xinput['ds'].dt.month==11)).astype(int)*4  # conversion bool => int\n",
    "matrix_correlation = Xinput[['saison'] + cols].groupby(['saison']).corr() \n",
    "matrix_correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatMap pour un meilleur visuel\n",
    "#.loc[1]=hiver\n",
    "#.loc[2]=printemps\n",
    "#.loc[3]=été\n",
    "#.loc[4]=automne\n",
    "plt.imshow(matrix_correlation.loc[1].as_matrix(),cmap='PuBu_r', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Que pensez-vous de ces corrélations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la consommation en fonction de la température de la station Paris-Montsouris\n",
    "On voudrait savoir si la consommation nationale peut s'expliquer en regardant simplement la température de la station du Parc Montsouris et en ignorant ce qui est extérieur au périphérique (Paris étant le centre du monde). Pour cela, on peut tracer un nuage de points.\n",
    "\n",
    "NB : Paris Montsouris est la station météo n°156\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xinput['X156Th_prev'], Yconso['y'], alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Que pensez-vous de ce nuage ? Est-ce suffisant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bricolage d'un modèle prédictif naïf\n",
    "\n",
    "<img src=\"pictures/hommeNaif.png\" width=500 height=60>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour se faire les dents on va considérer juste un point horaire\n",
    "datetime_a_predire = datetime.datetime.strptime(\"2016-12-20_14:00\", \"%Y-%m-%d_%H:%M\")\n",
    "y_true_point_horaire_cible = float(Yconso.loc[Yconso['ds'] == datetime_a_predire]['y'])\n",
    "\n",
    "print(\"On veut predire la consommation du {}, soit {}\".format(datetime_a_predire, y_true_point_horaire_cible))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Première idée, un modèle naïf : pour l'heure qui nous intéresse, on plaque bêtement la valeur de consommation nationale de la veille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par juste notre point horaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_modele_naif_1 = float(Xinput.loc[Xinput['ds'] == datetime_a_predire]['lag1D'])\n",
    "pred_error = abs(y_true_point_horaire_cible - y_pred_modele_naif_1)\n",
    "\n",
    "print(\"Modele 1 -- pred: {}, realisee: {}, erreur: {}%\".format(y_pred_modele_naif_1, y_true_point_horaire_cible, pred_error/y_true_point_horaire_cible * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant ce que ça donne non plus sur un unique point horaire mais sur l'ensemble des points horaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_modele_naif_1 = Xinput[\"lag1D\"]\n",
    "\n",
    "# On ignore les 24 premières heures à cause des NaN suite du début\n",
    "pred_error = (np.abs(Yconso[\"y\"].loc[24:] - y_pred_modele_naif_1.loc[24:]) / Yconso[\"y\"].loc[24:] * 100)\n",
    "\n",
    "print(np.mean(pred_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon c'est pas fou..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deuxième idée avec de l'expertise : pareil, avec comme raffinement le fait que l'on considere maintenant l'influence de la temperature\n",
    "\n",
    "<img src=\"pictures/ExpertJamy.jpg\" width=500 height=60>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_MW_par_degre = 2400  # par expertise, \n",
    "                           # on considere qu'une augmentation moyenne de 1°C \n",
    "                           # conduit à une augmentation de 2400MW de la conso nationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par juste notre point horaire préféré"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_real_veille = float(Xinput.loc[Xinput['ds'] == datetime_a_predire]['FranceTh_real_24h_avant'])\n",
    "temperature_prevu_cible = float(Xinput.loc[Xinput['ds'] == datetime_a_predire]['FranceTh_prev'])\n",
    "delta_temp = temperature_prevu_cible - temperature_real_veille\n",
    "delta_MW_because_temp = delta_temp * delta_MW_par_degre\n",
    "\n",
    "y_pred_modele_naif_2 = float(Xinput.loc[Xinput['ds'] == datetime_a_predire]['lag1D']) + delta_MW_because_temp\n",
    "pred_error = abs(y_true_point_horaire_cible - y_pred_modele_naif_2)\n",
    "\n",
    "print(\"Modele 2 -- pred: {}, realisee: {}, erreur: {}%\".format(y_pred_modele_naif_2, y_true_point_horaire_cible, pred_error/y_true_point_horaire_cible * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et maintenant sur l'ensemble des points horaires :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = Xinput[\"lag1D\"]\n",
    "\n",
    "delta_temp = Xinput['FranceTh_prev'] - Xinput['FranceTh_real_24h_avant']\n",
    "delta_MW_because_temp = delta_temp * delta_MW_par_degre\n",
    "\n",
    "y_pred_modele_naif_2 = Xinput[\"lag1D\"] + delta_MW_because_temp\n",
    "pred_error = (np.abs(Yconso[\"y\"].loc[24:] - y_pred_modele_naif_2.loc[24:]) / Yconso[\"y\"].loc[24:] * 100)\n",
    "\n",
    "print(np.mean(pred_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bon... En fait l'hypothèse comme quoi la consommation augmente de 2400 MW quand on perd 1°C ne tient que quand les températures sont basses, et non quand elles sont douces. D'ailleurs en période de canicule, à cause des climatiseurs, une augmentation de la température peut entrainer une augmentation de la consommation.\n",
    "\n",
    "Bien essayé avec ces modèles naïfs, mais maintenant on va être plus sérieux !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Préparer un jeu d'entrainement et un jeu de test\n",
    "En machine learning, il y a 2 types d'erreur que l'on peut calculer : l'erreur d'entrainement et l'erreur de test. \n",
    "\n",
    "Pour évaluer la capacité de notre modèle à bien généraliser sur de nouvelles données, il est très important de se préserver un jeu de test indépendant de celui d'entrainement.\n",
    "\n",
    "Il faut donc segmenter notre dataset en 2 : \n",
    "- un premier jeu servira pour l'entrainement, \n",
    "- tandis que le second servira à mesurer les performances du modèle prédictif."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDataSetEntrainementTest(Xinput, Yconso, dateDebut, dateRupture, nbJourlagRegresseur=0):\n",
    "    \n",
    "    dateStart = Xinput.iloc[0]['ds']\n",
    "    \n",
    "    DateStartWithLag = dateStart + pd.Timedelta(str(nbJourlagRegresseur)+' days')  #si un a un regresseur avec du lag, il faut prendre en compte ce lag et commencer l'entrainement a la date de debut des donnees+ce lag\n",
    "    XinputTest = Xinput[(Xinput.ds >= dateRupture)]    \n",
    "\n",
    "    XinputTrain=Xinput[(Xinput.ds < dateRupture) & (Xinput.ds > DateStartWithLag) & (Xinput.ds > dateDebut)]\n",
    "    YconsoTrain=Yconso[(Yconso.ds < dateRupture) & (Yconso.ds > DateStartWithLag) & (Yconso.ds > dateDebut)]\n",
    "    YconsoTest=Yconso[(Xinput.ds >= dateRupture)]\n",
    "    \n",
    "    return XinputTrain, XinputTest, YconsoTrain, YconsoTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on souhaite un jeu de test qui commence à partir du 1er mai 2017\n",
    "dateDebut = datetime.datetime(year=2013, month=1, day=7)#pour éviter les NaN dans le jeu de données\n",
    "dateRupture = datetime.datetime(year=2017, month=5, day=1)#début du challenge prevision de conso\n",
    "\n",
    "# On va commencer par un modèle autoregressif très simple, ici X=Y\n",
    "# Pas de prise en compte de la météo, des variables calendaires, etc...\n",
    "# Attention, on conserve dans un autre objet la matrice des variables exogènes\n",
    "Xinput_save = Xinput\n",
    "Xinput = Yconso \n",
    "nbJourlagRegresseur = 0  # pas de prise en compte des consommations passées pour l'instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XinputTrain, XinputTest, YconsoTrain, YconsoTest = prepareDataSetEntrainementTest(Xinput, Yconso, \n",
    "                                                                                  dateDebut, dateRupture, \n",
    "                                                                                  nbJourlagRegresseur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('la taille de l échantillon XinputTrain est:' + str(XinputTrain.shape))\n",
    "print('la taille de l échantillon XinputTest est:' + str(XinputTest.shape))\n",
    "print('la taille de l échantillon YconsoTrain est:' + str(YconsoTrain.shape))\n",
    "print('la taille de l échantillon YconsoTest est:' + str(YconsoTest.shape))\n",
    "print(\"la proportion de data d'entrainement est de:\" + str(YconsoTrain.shape[0] / (YconsoTrain.shape[0] + YconsoTest.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions utilitaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons la fonction modelError qui va calculer pour un échantillon (Y, Y_hat) différents scores :\n",
    "- erreur relative moyenne (MAPE en %)\n",
    "- erreur relative max (en %)\n",
    "- rmse (en MW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelError(Y, Yhat):\n",
    "\n",
    "    Y = Y.reset_index(drop=True)\n",
    "    \n",
    "    relativeErrorsTest = np.abs((Y['y'] - Yhat) /Y['y']) \n",
    "    errorMean = np.mean(relativeErrorsTest)\n",
    "    errorMax = np.max(relativeErrorsTest)\n",
    "    rmse = np.sqrt(mean_squared_error(Y['y'], Yhat))\n",
    "   \n",
    "    return relativeErrorsTest, errorMean, errorMax, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(YTrain, YTest, YTrainHat, YTestHat):\n",
    "    # Ytrain et Ytest ont deux colonnes : ds et y\n",
    "    # YtrainHat et YTestHat sont des vecteurs\n",
    "    ErreursTest, ErreurMoyenneTest, ErreurMaxTest, RMSETest = modelError(YTest, YTestHat)\n",
    "    print(\"l'erreur relative moyenne de test est de:\" + str(round(ErreurMoyenneTest*100,1))+\"%\")\n",
    "    print(\"l'erreur relative max de test est de:\" + str(round(ErreurMaxTest*100,1)) +\"%\")\n",
    "    print('le rmse de test est de:' + str(round(RMSETest,0)))\n",
    "    print()\n",
    "    ErreursTest, ErreurMoyenneTest, ErreurMaxTest, RMSETest = modelError(YTrain, YTrainHat)\n",
    "    print(\"l'erreur relative moyenne de train est de:\" + str(round(ErreurMoyenneTest*100,1))+\"%\")\n",
    "    print(\"l'erreur relative max de train est de:\" + str(round(ErreurMaxTest*100,1)) +\"%\")\n",
    "    print('le rmse de test est de:' + str(round(RMSETest,0))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_par(X, Y, Yhat,avecJF=True):\n",
    "    Y['weekday'] = Y['ds'].dt.weekday\n",
    "    Y['hour'] = Y['ds'].dt.hour\n",
    "    if(avecJF):\n",
    "        Y['JoursFeries'] = X['JoursFeries']\n",
    "    Y['APE'] = np.abs(Y['y']-Yhat)/Y['y']\n",
    "    dataWD = Y[['weekday','APE']]\n",
    "    groupedWD = dataWD.groupby(['weekday'], as_index=True)\n",
    "    statsWD = groupedWD.aggregate([np.mean])\n",
    "    dataHour = Y[['hour','APE']]\n",
    "    groupedHour = dataHour.groupby(['hour'], as_index=True)\n",
    "    statsHour = groupedHour.aggregate([np.mean])\n",
    "    \n",
    "    if(avecJF):\n",
    "        dataJF = Y[['JoursFeries','APE']]\n",
    "        groupedJF = dataJF.groupby(['JoursFeries'], as_index=True)\n",
    "        statsJF = groupedJF.aggregate([np.mean])\n",
    "    else:\n",
    "        statsJF = None\n",
    "    \n",
    "    return statsWD, statsHour, statsJF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Créer un modèle avec Prophet\n",
    "Vous allez utiliser la librairie Prophet developpée par facebook.: https://research.fb.com/prophet-forecasting-at-scale/. Elle a été publiée en 2017 et permet de faire des modèles de prévision sur des séries temporelles. En particulier, ces modèles captent surtout des saisonnalités, et peuvent également tenir compte de jours particuliers comme les jours fériés. Il est possible de rajouter d'autre variables explicatives selon un modèle statistique linéaire.\n",
    "\n",
    "C'est une librairie relativement ergonomique et performante en terme de temps de calculs d'où son choix ici.\n",
    "Un des aspects intéressant également est qu'elle repose sur un language probabiliste PyStan. Il est ainsi possible de décrire des variables selon une loi dans notre modèle et d'obtenir sans plus de développement des intervalles de confiance et incertitudes.\n",
    "\n",
    "Pour un tutoriel bien fait pour comprendre et utiliser Prophet, je vous recommande le lien suivant: http://www.degeneratestate.org/posts/2017/Jul/24/making-a-prophet/\n",
    "\n",
    "La prophétie autoréalisatrice: Marc Zuckerberg futur Président ..??\n",
    "\n",
    "<img src=\"pictures/zuckerbergProphet.jpg\" width=500 height=30>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer un modèle prophet avec une saisonnalité journalière et une tendance nulle pour la consommation\n",
    "mTrain = Prophet(daily_seasonality=True, n_changepoints=0)  # on considere une tendance relativement constante pour la consommation sur les 4 ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainer un modèle\n",
    "Notre modèle a des paramètres tels que les saisonnalités qu'il va falloir maintenant apprendre au vu de notre jeu d'entrainement. Il faut donc caler notre modèle sur ce jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mTrain.fit(XinputTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire des prédictions\n",
    "Une fois qu'un modèle de prévision est entrainé, il ne s'avère utile que s'il est performant sur de nouvelles situations. Faisons une prévision sur notre jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastTest = mTrain.predict(XinputTest)\n",
    "forecastTrain = mTrain.predict(XinputTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise nos previsions avec incertitudes\n",
    "dateavantRupture = dateRupture - pd.Timedelta('30 days')  # pour visualiser aussi les réalisations d'avant\n",
    "\n",
    "print('on plot a partir de la date:' + str(dateavantRupture))\n",
    "mTrain.history = mTrain.history[mTrain.history.ds >= dateavantRupture]  # pour demander à Prophet de ne plotter que notre période d'interet\n",
    "mTrain.plot(forecastTest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser le modèle\n",
    "Prophet dispose de méthodes de visualisation qui permettent d'interpreter le modèle appris, en particulier d'un point de vue des saisonalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise notre modele avec ses saisonalites\n",
    "mTrain.plot_components(forecastTest)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreter le modèle \n",
    "Au vu des visualisations précédentes :\n",
    "- quelles interprétations pouvez-vous faire du modèle?\n",
    "- Comment varie le comportement de la courbe de consommation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#avecJF=False#on n a pas encore considere de jours feries\n",
    "#evalWD,evalHour,evalJF = evaluation_par(XinputTest,YconsoTest,forecastTest['yhat'],avecJF)\n",
    "#print(str(round(evalWD*100,1)))\n",
    "#print(str(round(evalHour*100,1)))\n",
    "#print(str(round(evalJF*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer l'erreur de prévision\n",
    "Au vu de ces previsions faites par notre modèle sur de nouvelles situations, quelle est la performance de notre modèle sur ce jeu de test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(YconsoTrain, YconsoTest, forecastTrain['yhat'], forecastTest['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise nos previsions par rapport a la realité\n",
    "plt.plot(YconsoTest['ds'], YconsoTest['y'], 'b')\n",
    "plt.plot(forecastTest['ds'], forecastTest['yhat'], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enquêter autour des erreurs de prévision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation en fonction du jour de semaine, de l'heure, si jour férié ou non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment se distribue l'erreur ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erreur_relative_test, erreur_moyenne_test, erreur_max_test, rmse = modelError(YconsoTest, forecastTest['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "plt.hist(erreur_relative_test, num_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quel moment se trompe-t-on le plus ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecastTest['ds'], erreur_relative_test, 'r')\n",
    "plt.title(\"erreur relative sur la periode de test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.18\n",
    "\n",
    "mask = (erreur_relative_test >= threshold)\n",
    "forecastTest['ds'].loc[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Quelles variables explicatives peuvent nous permettre de créer un modele plus perfomant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On quitte Prophet pour d'autres modèles : RandomForest et XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation de Xinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput = Xinput_save\n",
    "Xinput = Xinput.drop(['lag1H'],axis=1)  # on supprime la consommation retardée d'une heure, non disponible pour notre exercice de prévision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xinput.shape)\n",
    "print(Xinput.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On encode les données calendaires en one-hot encoding pour le modèle.\n",
    "Cet encodage est nécessaire pour que le modèle mathématique puisse appréhender la notion de date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedWeekDay = pd.get_dummies(Xinput['weekday'],prefix=\"weekday\")\n",
    "encodedMonth = pd.get_dummies(Xinput['month'],prefix=\"month\")\n",
    "encodedHour = pd.get_dummies(Xinput['hour'],prefix=\"hour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedWeekDay.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedMonth.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedHour.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput = pd.concat([Xinput, encodedMonth, encodedWeekDay, encodedHour], axis=1)\n",
    "Xinput = Xinput.drop(['month','weekday','hour','saison'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xinput.shape)\n",
    "print(Xinput.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des prévisions météo à J+1 pour la veille\n",
    "colsToKeepWeather = [s for s in Xinput.columns.get_values() if 'Th_prev' in s]\n",
    "lag_colsToKeepWeather = [ s + \"_J_1\" for s in colsToKeepWeather ]\n",
    "Xinput[lag_colsToKeepWeather] = Xinput[colsToKeepWeather].shift(24)\n",
    "time = pd.to_datetime(Xinput['ds'], yearfirst=True)\n",
    "Xinput['posan']= time.dt.dayofyear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des jours fériés dans Xinput\n",
    "encodedHolidays = pd.get_dummies(Xinput[['holiday']], prefix = \"JF\")\n",
    "encodedHolidays['JoursFeries'] = encodedHolidays.sum(axis = 1)\n",
    "Xinput = pd.concat([Xinput, encodedHolidays], axis = 1)\n",
    "Xinput = Xinput.drop(['holiday'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affichage de toutes les variables de base\n",
    "list(Xinput) #list plutôt que print pour avoir la liste complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XinputTrain, XinputTest, YconsoTrain, YconsoTest = prepareDataSetEntrainementTest(Xinput, \n",
    "                                                                                  Yconso, \n",
    "                                                                                  dateDebut, \n",
    "                                                                                  dateRupture, \n",
    "                                                                                  nbJourlagRegresseur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape de XinputTrain est:' + str(XinputTrain.shape[0]))\n",
    "print('shape de XinputTest est:' + str(XinputTest.shape[0]))\n",
    "print('shape de YconsoTrain est:' + str(YconsoTrain.shape[0]))\n",
    "print('shape de YconsoTest est:' + str(YconsoTest.shape[0]))\n",
    "print('la proportion de data d entrainement est de:' + str(YconsoTrain.shape[0] / (YconsoTrain.shape[0] + YconsoTest.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle RandomForest\n",
    "\n",
    "<img src=\"pictures/randomForestExplain.png\" width=500 height=30>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation des données d'entrée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToKeepWeather = [s for s in Xinput.columns.get_values() if 'Th_prev' in s]\n",
    "colsToKeepMonth = [v for v in Xinput.columns.get_values() if 'month' in v]\n",
    "colsToKeepWeekday = [v for v in Xinput.columns.get_values() if 'weekday' in v]\n",
    "colsToKeepHour = [v for v in Xinput.columns.get_values() if 'hour' in v]\n",
    "colsToKeepHolidays = [v for v in Xinput.columns.get_values() if 'JF_' in v]\n",
    "\n",
    "colsRF = np.concatenate((['lag1D','lag1W','JoursFeries'],\n",
    "                         colsToKeepWeather,colsToKeepMonth,colsToKeepWeekday,colsToKeepHour))\n",
    "list(colsRF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La cellule peut prendre un peu de temps à exécuter\n",
    "rfTrain = RandomForestRegressor(n_estimators=30, max_features=colsRF.size, n_jobs=3)\n",
    "rfTrain.fit(XinputTrain[colsRF], YconsoTrain['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecastTest = rfTrain.predict(XinputTest[colsRF])\n",
    "forecastTrain = rfTrain.predict(XinputTrain[colsRF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(YconsoTrain, YconsoTest, forecastTrain, forecastTest)\n",
    "\n",
    "# on visualise nos previsions par rapport a la realité\n",
    "plt.plot(YconsoTest['ds'], YconsoTest['y'], 'b', YconsoTest['ds'], forecastTest, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalWD,evalHour,evalJF = evaluation_par(XinputTest,YconsoTest,forecastTest)\n",
    "print(str(round(evalWD*100,1)))\n",
    "print(str(round(evalHour*100,1)))\n",
    "print(str(round(evalJF*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle xgboost\n",
    "\n",
    "<img src=\"pictures/XGboost.png\" width=500 height=30>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbTrain = xgb.XGBRegressor( )\n",
    "xgbTrain.fit(XinputTrain[colsRF], YconsoTrain['y'])\n",
    "forecastTestXGB = xgbTrain.predict(XinputTest[colsRF])\n",
    "forecastTrainXGB = xgbTrain.predict(XinputTrain[colsRF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation(YconsoTrain, YconsoTest, forecastTrainXGB, forecastTestXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalWD,evalHour,evalJF = evaluation_par(XinputTest,YconsoTest,forecastTestXGB)\n",
    "print(str(round(evalWD*100,1)))\n",
    "print(str(round(evalHour*100,1)))\n",
    "print(str(round(evalJF*100,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Selon vous, pourquoi l'erreur max est significative pour tous les modèles ?\n",
    "- Comment y remédier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: à vous de jouer\n",
    "\n",
    "Bravo ! Vous avez déjà créé un premier modèle performant pour faire des prévisions sur une fenêtre glissante à horizon 24h !\n",
    "\n",
    "Maintenant à vous de mettre votre expertise pour créer de nouveaux modèles.\n",
    "\n",
    "Vous pouvez continuer à explorer le problème selon plusieurs axes:\n",
    "- créer des modèles pour les régions françaises\n",
    "- tester votre modèle sur une autre saison (l'hiver par exemple)\n",
    "- créer de nouvelles variables explicatives ? Quid de la météo et de la température? Des jours fériés ? Du feature engineering plus complexe...\n",
    "- détecter des outliers dans les données\n",
    "- etudiez les incertitudes et les possibilités offertes par PyStan\n",
    "\n",
    "Mettez-vous en 3 groupes, explorez pendant 30 minutes, et restituez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
