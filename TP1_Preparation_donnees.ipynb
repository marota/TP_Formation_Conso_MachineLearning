{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP facultatif : Préparation du jeu de données brut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quand on se lance avec enthousiasme dans un nouveau projet de machine-learning, on pense avant tout au choix du modèle que l'on va utiliser. Cependant, et même s'il ne s'agit pas de l'étape la plus agréable du travail, un préalable indispensable est de réunir les données brutes et de les mettre en forme pour qu'elle puisse être ingérée par le modèle.\n",
    "\n",
    "L'objectif de ce TP est de comprendre comment, à partir de différentes sources de données, on construit nos jeux de données \"propres\" pour ensuite construire le meilleur modèle d'apprentissage possible pour la prévision de consommation nationale.\n",
    "\n",
    "Nos fichiers d'entrée bruts sont les suivants :\n",
    "> * YconsoT0.csv\n",
    "> * joursFeries.csv\n",
    "> * StationsMeteoRTE.csv  # Les coordonnées géographiques et les poids associés aux stations météos\n",
    "> * meteoX_T0_T24.zip\n",
    "> * eCO2mix_RTE_tempo_2017-2018.xls  # le sinformations sur les jours TEMPO\n",
    "\n",
    "Et les fichiers que l'on va créer sont :\n",
    "> * Xinput.csv  # Les entrées pour le modèle d'apprentissage\n",
    "> * Yconso.csv  # les sorties pour le modèle d'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnement\n",
    "\n",
    "Chargement des librairies python, et quelques éléments de configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécutez la cellule ci-dessous (par exemple avec shift-entrée)\n",
    "# Si vous exécuter ce notebook depuis votre PC, il faudra peut-etre installer certaines librairies avec \n",
    "# 'pip3 install ma_librairie'\n",
    "import os  # accès aux commandes système\n",
    "import datetime  # structure de données pour gérer des objets calendaires\n",
    "import pandas as pd  # gérer des tables de données en python\n",
    "import numpy as np  # librairie d'opérations mathématiques\n",
    "import zipfile # manipulation de fichiers zip\n",
    "import urllib3 # téléchargement de fichier\n",
    "\n",
    "%autosave 0\n",
    "\n",
    "data_folder = os.path.join(os.getcwd(),\"data\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Mon repertoire de data est : {}\".format(data_folder))\n",
    "print(\"\")\n",
    "print(\"Fichiers contenus dans ce répertoire :\")\n",
    "for file in os.listdir(data_folder):\n",
    "    print(\" - \" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données\n",
    "\n",
    "Dans cette partie nous allons charger les fichiers csv nécessaires pour l'analyse, puis les convertir en data-frame python. \n",
    "\n",
    "Les données de base à récupérer sont :\n",
    "- Les historiques de consommation\n",
    "- Le calendrier des jours fériés\n",
    "- Les données météo, ainsi que la liste des stations\n",
    "- Le calendrier des jours TEMPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Données de consommation\n",
    "\n",
    "Dans un premier temps on importe les données de consommation réalisée à partir du fichier \"YconsoT0.csv\". La date et l'heure sont données dans les deux premières colonnes, et les autres colonnes correspondent aux consommations des 12 régions françaises (hors Corse) et à la consommation nationale.\n",
    "\n",
    "Pour cela on utilise la bibliothèque **pandas** pour la manipulation de données et la fonction **read_csv**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import depuis un csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les données du csv sont importé dans un objet de type dataframe\n",
    "conso_csv = os.path.join(data_folder, \"YconsoT0.csv\")\n",
    "conso_df = pd.read_csv(conso_csv, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut ensuite vérifier que les données sont importées correctement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les dimensions et le noms des colonnes de la data frame\n",
    "print(conso_df.shape)  # Nombre de lignes, nombre de colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des colonnes de la data-frama\n",
    "print(conso_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des premières lignes\n",
    "print(conso_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Petit détour pour gérer les dates\n",
    "\n",
    "Le fichier YconsoT0.csv contient en particulier 2 colonnes 'date' et 'time'. Celles-ci contiennent des objets de type \"string\" correspondant à la date et à l'heure. \n",
    "\n",
    "<img src=\"pictures/clock.png\" width=60 height=60>\n",
    "\n",
    "Nous allons fusionner ces informations en une nouvelle colonne d'objets de type **datetime** mieux adaptés pour la manipulation de dates et d'heures. En effet, pour manipuler des dates (effectuer des tris, des sélections, récupérer si c'est un lundi, mardi,...), il est plus efficace de passer par un objet \"datetime\" plutôt que de se débrouiller en manipulant des chaînes de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On appelle \"ds\" (dateStamp) cette nouvelle colonne\n",
    "conso_df['ds'] = pd.to_datetime(conso_df['date'] + \" \" + conso_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conso_df[['ds', 'date', 'time']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule ci-dessous a pour but d'illustrer comment utiliser ces objets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime vers string\n",
    "noel_2017_date = datetime.date(2017, 12, 25)\n",
    "noel_2017_str = datetime.datetime.strftime(noel_2017_date, format=\"%Y-%m-%d\")\n",
    "print(\"noel_2017_date vaut : {}, et est de type {}\".format(noel_2017_date, str(type(noel_2017_date))))\n",
    "print(\"noel_2017_str vaut : {}, et est de type {}\".format(noel_2017_str, str(type(noel_2017_str))))\n",
    "print(\"---\")\n",
    "\n",
    "# string vers datetime\n",
    "starwars_day_2017_str = \"2017-05-04\"\n",
    "starwars_day_2017_date = datetime.datetime.strptime(starwars_day_2017_str, \"%Y-%m-%d\")\n",
    "print(\"starwars_day_2017_date vaut : {}, et est de type {}\".format(starwars_day_2017_date, str(type(starwars_day_2017_date))))\n",
    "print(\"D'ailleurs, c'était le \" + str(starwars_day_2017_date.weekday() + 1) + \" ème jour de la semaine, où 0 correspond à lundi et 6 correspond à dimanche\")\n",
    "print(\"starwars_day_2017_str vaut : {}, et est de type {}\".format(starwars_day_2017_str, str(type(starwars_day_2017_str))))\n",
    "print(\"---\")\n",
    "\n",
    "# Voyager dans le temps\n",
    "saint_sylvestre_2017_date = datetime.date(2017, 12, 31)\n",
    "bienvenu_en_2018_date = saint_sylvestre_2017_date + datetime.timedelta(days=1)\n",
    "print(\"Le 31 décembre 2017 plus un jour ça donne le {}\".format(bienvenu_en_2018_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Réduction du problème : se débarrasser des données qui ne nous intéressent pas\n",
    "\n",
    "Le dataframe de consommation est volumineux, et contient beaucoup d'information inutile (au moins en première approximation) pour notre problème de prévision de la consommation nationale. On va donc le simplifier.\n",
    "\n",
    "On va se concentrer sur la consommation à l'**échelle nationale** au **pas horaire**. On va donc ne conserver que la colonne qui nous intéresse, et ne conserver que les lignes qui correspondent aux heures pleines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on commence par ecarter les colonnes inutiles\n",
    "conso_france_df = conso_df[['ds', 'Consommation.NAT.t0']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# et maintenant on ne garde que les heures pleines\n",
    "# Pour cela on va utiliser notre colonne d'objet datetime\n",
    "minutes = conso_france_df['ds'].dt.minute\n",
    "indices_hours = np.where(minutes.values == 0.0)\n",
    "\n",
    "#print(conso_france_df['ds'])\n",
    "#print(minutes)\n",
    "#print(indices_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conso_france_horaire_df = conso_france_df.loc[indices_hours]\n",
    "conso_france_horaire_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# les index de ce sous-dataframe correspondent à celle du dataframe de base,\n",
    "# et donc sont pour l'instant des multiples de 4.\n",
    "# on va les réinitialiser pour avoir une dataframe \"neuve\"\n",
    "conso_france_horaire_df = conso_france_horaire_df.reset_index(drop=True)  \n",
    "print(conso_france_horaire_df.head(5))\n",
    "print(conso_france_horaire_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récuperation des jours fériés\n",
    "\n",
    "Même principe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_feries_csv = os.path.join(data_folder,\"joursFeries.csv\")\n",
    "jours_feries_df = pd.read_csv(jours_feries_csv, sep=\";\")\n",
    "\n",
    "jours_feries_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour la première colonne, les dates sont au format \"string\"\n",
    "# Nous allons les convertir en objet \"datetime\" mieux adaptés pour la manipulation de dates\n",
    "print(\"Après import du csv, la colonne ds est de type \" + str(type(jours_feries_df.ds[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_feries_df.ds = pd.to_datetime(jours_feries_df.ds)\n",
    "print(\"maintenant, la colonne ds est de type \" + str(type(jours_feries_df.ds[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jours_feries_df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette dataframe fait correspondre le timestamp \"2012-12-25\" avec \"Noël\". Cependant, le timestamp \"2012-12-25\" correspond implicitemlent au timestamp \"2012-12-25 00:00\", ce qui fait que pour l'instant les timestamp \"2012-12-25 00:05\" ou \"2012-12-25 03:00\" ne sont pas identifiés comme étant aussi Noël, ce qui va poser problème plus tard.\n",
    "\n",
    "La cellule ci-dessous va étendre la dataframe des jours fériés de sorte à résoudre ce problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps_of_interest = conso_df[['ds']]\n",
    "timestamps_of_interest[\"day\"] = timestamps_of_interest[\"ds\"].apply(lambda x: datetime.datetime.strftime(x, format=\"%Y-%m-%d\"))\n",
    "\n",
    "tmp_df = jours_feries_df\n",
    "tmp_df[\"day\"] = jours_feries_df[\"ds\"].apply(lambda x: datetime.datetime.strftime(x, format=\"%Y-%m-%d\"))\n",
    "\n",
    "tmp_df = pd.merge(timestamps_of_interest, tmp_df, on='day', how=\"left\", suffixes=(\"\", \"_tmp\"))\n",
    "\n",
    "jours_feries_df = tmp_df[[\"ds\", \"holiday\"]]\n",
    "print(jours_feries_df.loc[450:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Récupération des coordonnées géographiques des stations météo - au boulot !\n",
    "\n",
    "On va charger le csv qui à chaque station météo attribue sa longitude/latitude/poids. Pour en savoir plus sur les poids :  \n",
    "https://clients.rte-france.com/lang/fr/visiteurs/services/actualites.jsp?id=9482&mode=detail\n",
    "\n",
    "**Votre mission** :\n",
    "- Importez les données contenues dans le fichier csv *StationsMeteoRTE.csv* qui se situe dans data_folder vers un dataframe *stations_meteo_df*\n",
    "- Regardez à quoi ces données ressemblent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargez les données de StationsMeteoRTE.csv vers stations_meteo_df\n",
    "stations_meteo_csv = os.path.join(data_folder, \"StationsMeteoRTE.csv\")\n",
    "stations_meteo_df = pd.read_csv(stations_meteo_csv, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meteo_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour compter le nombre de stations il suffit de compter le nombre de lignes dans le data-frame\n",
    "# Ceci se fait un utilisant \"shape\"\n",
    "nb_stations = stations_meteo_df.shape[0]\n",
    "print(nb_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération du dataframe de météo\n",
    "\n",
    "<img src=\"pictures/weather.png\" width=60 height=60>\n",
    "\n",
    "On va utiliser les mêmes fonctions que précédemment pour lire le fichier **'meteoX_T.csv'**, qui est situé dans data_folder et contient les historiques de température réalisée et prévue pour différentes stations Météo France.\n",
    "\n",
    "**Attention : Les données météo sont encryptées dans un fichier zip.**  \n",
    "Pour les lire vous avez besoin d'un mot de passe qui ne peut vous être donné que dans le cadre d'un travail au sein de RTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_zip = os.path.join(data_folder, \"meteoX_T0_T24.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "password = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette étape peut être un peu longue car le fichier est volumineux\n",
    "\n",
    "# Pour travailler avec les fichiers zip, on utilise la bibliothèque **zipfile**.\n",
    "zipfile_meteo = zipfile.ZipFile(meteo_zip)\n",
    "zipfile_meteo.setpassword(bytes(password,'utf-8'))\n",
    "meteo_df = pd.read_csv(zipfile_meteo.open('meteoX_T0_T24'),sep=\";\",engine='c',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On se crée une colonne avec des objets timestamp pour les dates\n",
    "meteo_df['ds'] = pd.to_datetime(meteo_df['date'] + ' ' + meteo_df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meteo_df.shape)  # (nb lignes , nb_colonnes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meteo_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme pour la consommation, on ne retient que les données des heures rondes afin de réduire la taille du problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "minutes = meteo_df['ds'].dt.minute\n",
    "mask = np.where(minutes.values == 0.0)\n",
    "meteo_horaire_df = meteo_df.loc[mask]\n",
    "\n",
    "# On remet les index au propre\n",
    "meteo_horaire_df = meteo_horaire_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour se mettre dans le cadre d'un exercice de prévision, et toujours dans l'idée de réduire la taille du problème, on ne va conserver que les températures réalisées, les températures prévues à 24h (noms de colonnes finissant par 'Th+24'), ainsi que la colonne _ds_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_garder = ['ds'] + list(meteo_horaire_df.columns[meteo_horaire_df.columns.str.endswith(\"Th+0\")]) + list(meteo_horaire_df.columns[meteo_horaire_df.columns.str.endswith(\"Th+24\")])\n",
    "meteo_prev_df = meteo_horaire_df[colonnes_a_garder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meteo_prev_df.head(5))\n",
    "print(meteo_prev_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : récupération de données depuis internet\n",
    "\n",
    "Dans le but d'automatiser un processus, nous pouvons implémenter une fonction qui va chercher les dernières données mises à disposition sur internet.  \n",
    "\n",
    "Pour l'exemple de la prévision de consommation, il serait pertinent de fournir en entrée du modèle l'information sur le type de jour Tempo. Les clients ayant souscrit à ce type de contrat sont incités à réduire leur consommations les jours BLANC et ROUGE, aussi on peut penser que cette information permettra d'améliorer la qualité des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation à la main\n",
    "\n",
    "Avant d'implémenter la version automatique, faisons une fois à la main cette manipulation.\n",
    "\n",
    " - Recupérez à la main le calendrier TEMPO pour 2017-2018 :\n",
    " http://www.rte-france.com/fr/eco2mix/eco2mix-telechargement\n",
    " - Le déposer dans _data&#95;folder_\n",
    " - Le dézipper\n",
    " - Regarder les données dans excel ou autre. Notez en particulier la fin du fichier, la supprimer\n",
    " \n",
    "Importez ces données dans un dataframe avec 'read_excel' de la librairie pandas ou autre méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempo_xls = os.path.join(data_folder, \"eCO2mix_RTE_tempo_2017-2018.xls\")\n",
    "tempo_df = pd.read_csv(tempo_xls, sep=\"\\t\", encoding=\"ISO-8859-1\")  # ce fichier est en fait un csv et non un xls..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tempo_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La même chose automatisée\n",
    "\n",
    "On récupère maintenant automatiquement les informations sur Internet à partir de l'url, sans devoir les chercher à la main soi-même."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tempo_data(url, data_folder, tempo_xls_zip_name):\n",
    "    \n",
    "    tempo_xls_zip = os.path.join(data_folder, tempo_xls_zip_name)\n",
    "    \n",
    "    # Récupération du fichier zip depuis internet\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    http = urllib3.PoolManager()    \n",
    "    with http.request('GET', url, preload_content=False) as resp, open(tempo_xls_zip, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "        \n",
    "    with zipfile.ZipFile(tempo_xls_zip, \"r\") as zip_file:\n",
    "        zip_file.extractall(data_folder)\n",
    "\n",
    "    # Petite vérification\n",
    "    if not os.path.isfile(tempo_xls_zip):\n",
    "        print(\"ERROR!! {} not found in {}\".format(\"eCO2mix_RTE_tempo_2017-2018.xls\", data_folder))\n",
    "        raise RuntimeError(\"Tempo data not uploaded :-(\")\n",
    "\n",
    "    # Import de ces données dans un dataframe\n",
    "    tempo_df = pd.read_csv(tempo_xls_zip, sep=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "    # Suppression du disclaimer de la dernière ligne de tempo_df, par exemple avec la méthode drop d'un dataframe\n",
    "    last_row = len(tempo_df.index) - 1\n",
    "    tempo_df = tempo_df.drop(tempo_df.index[last_row])\n",
    "\n",
    "    return tempo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On teste la fonction définie ci-dessus. Parfois pour de sombres raisons de proxy la connection au serveur peut échouer. Comme ce TP porte sur le machine-learning on ne s'acharnera pas sur cette partie en cas d'échec :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://eco2mix.rte-france.com/curves/downloadCalendrierTempo?season=17-18\"\n",
    "#tempo_xls_zip_name = \"eCO2mix_RTE_tempo_2017-2018.zip\"\n",
    "\n",
    "#tempo_df = get_tempo_data(url, data_folder, tempo_xls_zip_name)\n",
    "\n",
    "#print(tempo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les personnes intéressées par le webscrapping, jeter un oeil du côté de <a href=\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\" title=\"link to google\">BeautifulSoup</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fusion des données\n",
    "\n",
    "<img src=\"pictures/fusion.png\" width=600 height=200>\n",
    "\n",
    "On va maintenant construire un dataframe unique qui regroupe toutes les données nécessaire à notre modèle de prévision. On aura ici une ligne pour chaque timestamp, et dans cette ligne à la fois notre X et notre Y pour le futur modèle de machine-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dans un premier temps, on fusionne la consommation et la température.\n",
    "merged_df = pd.merge(conso_france_horaire_df, meteo_prev_df, on = 'ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on fusionne avec le calendrier des jours fériés en joignant sur la colle \"ds\" des timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(merged_df, jours_feries_df, how = \"left\", on = \"ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul de la température France 32 villes \n",
    "\n",
    "On va ajouter une colonne à notre dataframe, colonne que - par expérience/expertise - on sait pouvoir être utile pour prévoir la consommation.\n",
    "\n",
    "La température France est une moyenne pondérée de la température de 32 stations. On a donc besoin des poids de stations_meteo_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "merged_df['FranceTh+0'] = np.dot(merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+0\")])], stations_meteo_df['Poids'])\n",
    "merged_df['FranceTh+24'] = np.dot(merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+24\")])], stations_meteo_df['Poids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.shape)\n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohérence temporelle des données pour notre modèle de prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons quelques instants pour regarder les données que l'on a pour l'instant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque point horaire, on a :\n",
    "* la consommation réalisée (au niveau national\n",
    "* la température réalisée pour chaque station météo, ainsi qu'une valeur représentative de la température moyenne à l'échelle nationale\n",
    "* une prévision de température pour 24 heures plus tard pour chaque station météo, ainsi qu'une prévision de la température moyenne France pour dans 24 heures\n",
    "* l'information si le point horaire appartient à un jour férié ou non    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce que l'on veut faire, c'est mettre au point un modèle qui prédit comme Y :\n",
    "* la consommation nationale pour point_horaire_cible\n",
    "prenant en entrée un X qui comporte tout ou partie de :\n",
    "* l'information si point_horaire_cible appartient à un jour férié\n",
    "* La prévision météo pour point_horaire_cible, prévision établie 24h à l'avance\n",
    "* La consommation réalisée 24h avant point_horaire_cible\n",
    "* La température réalisée 24h avant point_horaire_cible\n",
    "Il faut simplement être vigilant sur le fait qu'il est interdit de prédire une consommation pour point horaire cible en ayant comme entrée la température réalisée pour point horaire cible (on n'est pas dans minority report)\n",
    "\n",
    "Ainsi, on va adapter *merged_df* de sorte à ce que chaque ligne correspondent à un point horaire cible à prédire, avec en colonne :\n",
    "* le Y (la consommation nationale pour point_horaire_cible)\n",
    "* Le X (cf. ci-dessus)\n",
    "\n",
    "Ainsi, on va devoir décaler toutes les données de températures de 24 heures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+0\")])] = merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+0\")])].shift(24)\n",
    "merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+24\")])] = merged_df[list(merged_df.columns[merged_df.columns.str.endswith(\"Th+24\")])].shift(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par souci de clarté on renomme les colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [column.replace(\"Th+0\", \"Th_real_24h_avant\").replace(\"Th+24\", \"Th_prev\") for column in merged_df.columns]\n",
    "print(new_columns)\n",
    "\n",
    "merged_df.columns = new_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppression des NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head(3))\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : Attention il est normal que la colonne \"_holiday_\" comporte des NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ~merged_df[[\"FranceTh_real_24h_avant\"]].isnull().any(axis=1)\n",
    "merged_df = merged_df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merged_df.head(3))\n",
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde du fichier \n",
    "\n",
    "Tout d'abord on sépare les données en deux : \n",
    "- le vecteur de consommation à prévoir : y_conso\n",
    "- La matrice des variables explicatives : X_input\n",
    "\n",
    "Sachant que plus tard notre modèle aura pour mission d'établir une correspondance _f_ telle que l'on ait du mieux possible une relation *y = f(X)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conso = merged_df[['ds', 'Consommation.NAT.t0']]\n",
    "y_conso.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = merged_df.drop(['Consommation.NAT.t0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_conso.to_csv(\"data/Yconso.csv\", index = False)\n",
    "X_input.to_csv(\"data/Xinput.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et enfin on zip Xinput.csv avec un mot de passe.  \n",
    "Depuis un terminal :\n",
    "\n",
    "> zip -e Xinput.zip Xinput.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
