{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le problème\n",
    "\n",
    "Un tout nouveau data challenge vient d'être lancé sur la plateforme dataScience.net pour aider RTE a faire de meilleures prévisions de conso ! \n",
    "https://www.datascience.net/fr/challenge/33/details\n",
    "\n",
    "Le premier challenge a montré que des prévisionnistes externes pouvaient faire mieux que nos meilleurs prévisionnistes à RTE... nous voilà ringardisés ! \n",
    "Une deuxième chance nous est néanmoins offerte pour montrer que quand même on s'y connait en prévision de conso :).\n",
    "\n",
    "Nous voilà tous réunis pour prendre ce défi à bras le corps et redorer l'image de RTE. On compte sur vous !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un outil: Le Machine Learning\n",
    "Pour cela nous allons avoir recours au Machine Learning. Cela nous permettra de créer un modèle qui apprend et s'adapte au contexte sans programmer un système expert avec des \"centaines\" de règles en dur par de la programmation logique . Le machine learning nécessite toutefois de la connaissance experte dans le domaine d'intérêt pour créer des modèles pertinents et efficaces. En effet, si notre modèle embarque trop de variables peu explicatives, il sera noyé dans l'information, surapprendra sur les exemples qu'on lui a montrés, et aura du mal à généraliser en prédisant avec justesse sur de nouveaux exemples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Une difficulté: le feature engineering\n",
    "Au-delà de la simple sélection de variables pertinentes, on fait surtout ce que l'on appelle du feature engineering avec notre expertise: on créé des variables transformées ou aggrégées, comme une consommation moyenne sur le mois précédent ou une température moyenne sur la France, pour guider l'algorithme à apprendre sur l'information la plus pertinente et synthétique.\n",
    "Cela suppose de bien connaître nos données, de les traiter et de les visualiser avec différents algorithmes au préalable.\n",
    "\n",
    "Nous allons ici voir ce que cela implique en terme de développement et d'implémentation de participer à un tel challenge, en montrant les capacités du Machine Learning sur la base de modèles \"shallow\", et aussi leurs limites.\n",
    "\n",
    "## To be continued\n",
    "Le deuxième TP permettra d'investiguer les modèles \"Deep\" avec réseaux de neurones, en montrant le moindre besoin en feature engineering et leur plus grande capacité a absorber l'information de par les représentations hiérarchiques qu'ils se créent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ce qu'on va voir dans ce premier TP:\n",
    "1) formaliser le problème: que souhaite-t-on prédire (quel est mon Y) ? Avec quelles variables explicatives (quel est mon X) ?\n",
    "\n",
    "2) collecter les données: où se trouvent les données ? Quel est le format ? Comment les récupérer ?\n",
    "\n",
    "3) investiguer les données: visualiser des séries temporelles, faire quelques statistiques descriptives\n",
    "\n",
    "4) préparer les données: pour entrainer et tester un premier modèle\n",
    "\n",
    "5) créer et entrainer un premier modèle simple: ce sera notre baseline\n",
    "\n",
    "6) évaluer un modèle\n",
    "\n",
    "7) itérer en créant de nouveaux modèles avec de nouvelles variables explicatives\n",
    "\n",
    "8) Jouez: créer vos propres modèles, tester sur une saison différente, tester sur une région différente, faire une prévision avec incertitudes, détecter des outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionnement en temps\n",
    "On prévoit un TP d'environ 2h :\n",
    "- 20-30 minutes pour charger et préparer les données\n",
    "- 30-40 minutes pour analyser et visualiser les données\n",
    "- 45-60 minutes pour créer, entrainer, évaluer et interpréter les modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se familiariser avec le problème: Eco2mix\n",
    "Quand on parle de courbe de consommation France, il y a une application incontournable : eco2mix !\n",
    "Allons voir à quoi ressemblent ces courbes de consommation, pour nous faire une idée du problème et se donner quelques intuitions:\n",
    "http://www.rte-france.com/fr/eco2mix/eco2mix\n",
    "ou sur application mobile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécutez la cellule ci-dessous (par exemple avec shift-entrée)\n",
    "# Si vous exécuter ce notebook depuis votre PC, il faudra peut-etre installer certaiines librairies avec \n",
    "# 'pip install ma_librairie'\n",
    "import os  # accès aux commandes système\n",
    "import datetime  # structure de données pour gérer des objets calendaires\n",
    "import pandas as pd  # gérer des tables de données en python\n",
    "import numpy as np  # librairie d'opérations mathématiques\n",
    "import matplotlib.pyplot as plt  # tracer des visualisations\n",
    "import sklearn  # librairie de machine learning\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from fbprophet import Prophet  # un package de series temporelles mis a disposition par facebook\n",
    "import urllib3  # scrapper le web\n",
    "import shutil  # move ou copier fichier\n",
    "import zipfile  # compresser ou décompresser fichier\n",
    "\n",
    "# Pour visualisation sur une carte utilsons la librairie bokeh qui fait appel a une api GoogleMaps\n",
    "from bokeh.palettes import inferno\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, Range1d, PanTool, WheelZoomTool, BoxSelectTool, ColorBar, LogTicker,\n",
    "    LabelSet, Label,HoverTool\n",
    ")\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "from bokeh.models.mappers import LogColorMapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choix du répertoire de travail \"data_folder\" dans lequel tous les fichiers csv seront entreposés\n",
    "\n",
    "# @Antoine : a actualiser avec le repertoire par defaut sur le serveur\n",
    "data_folder = os.path.join(\"/local/partage/RTEConsoChallenge/data_challenge\")\n",
    "\n",
    "# Petite vérification\n",
    "print(\"Mon repertoire est : {}\".format(data_folder))\n",
    "print(\"Fichiers contenus dans ce répertoire :\")\n",
    "for file in os.listdir(data_folder):\n",
    "    print(\" - \" + file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération des données\n",
    "\n",
    "Dans cette partie nous allons charger les fichiers csv nécessaires pour l'analyse, puis les convertir en data-frame python.\n",
    "Les données de base à récupérer sont :\n",
    "- Les historiques de consommation\n",
    "- Le calendrier des jours fériés\n",
    "- Les données météo, ainsi que la liste des stations\n",
    "- (Bonus) Le calendrier des jours TEMPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recuperation des historiques de consommation\n",
    "\n",
    "# Conversion en tant que data-frame\n",
    "# Remarquez que l'on manipule un gros fichier, ce qui explique pourquoi l'exécution de cette cellule prend du temps\n",
    "conso_csv = os.path.join(data_folder, \"conso_Y.csv\")\n",
    "conso_df = pd.read_csv(conso_csv, sep=\";\", engine='c', header=0) #engine en language C et position header pour accélérer le chargement\n",
    "\n",
    "# Regardons la tête des données, méthode head() dans librairie pandas. 1 ligne\n",
    "# TODO\n",
    "#\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#afficher les dimensions et le noms des colonnes de la data frame. Appel de méthodes \"shape\" et \"columns\". 2 lignes\n",
    "# TODO\n",
    "#\n",
    "#\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- Quelles variables à prévoir ont été ici formatées ?\n",
    "- Quelles variables peuvent être utilisées pour faire les prévisions ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conso_Y.csv contient en particulier 2 colonnes 'date' et 'time', \n",
    "# les deux contenant des objets de type \"string\".\n",
    "# Nous allons fusionner ces informations en une nouvelle colonne\n",
    "# d'objets \"ds\" (dateStamp) mieux adaptés pour la manipulation de dates et d'hheures\n",
    "\n",
    "conso_df['ds'] = pd.to_datetime(conso_df['date']+' '+conso_df['time'])\n",
    "\n",
    "print(conso_df[['date','time','ds']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutoriel sur les méthodes datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime vers string\n",
    "noel_2017_date = datetime.date(2017, 12, 25)\n",
    "noel_2017_str = datetime.datetime.strftime(noel_2017_date, format=\"%Y-%m-%d\")\n",
    "print(\"noel_2017_date vaut : {} ; et est de type {}\".format(noel_2017_date, str(type(noel_2017_date))))\n",
    "print(\"noel_2017_str vaut : {} ; et est de type {}\".format(noel_2017_str, str(type(noel_2017_str))))\n",
    "\n",
    "# Convertir la date/heure de tout de suite en string avec le pattern donné\n",
    "pattern = \"%Y-%m-%d - %H:%M\"\n",
    "### DEBUT - TO DO\n",
    "now_date = datetime.datetime.now()\n",
    "now_str = \"A vous de jouer\"\n",
    "print(\"la_tout_de_suite_str vaut : {} ; et est de type {}\".format(now_str, str(type(now_str))))\n",
    "### FIN - TO DO\n",
    "\n",
    "# string vers datetime\n",
    "starwars_day_2017_str = \"2017-05-04\"\n",
    "starwars_day_2017_date =  datetime.datetime.strptime(starwars_day_2017_str, \"%Y-%m-%d\")\n",
    "print(\"starwars_day_2017_date vaut : {} ; et est de type {}\".format(starwars_day_2017_date, str(type(starwars_day_2017_date))))\n",
    "print(\"starwars_day_2017_str vaut : {} ; et est de type {}\".format(starwars_day_2017_str, str(type(starwars_day_2017_str))))\n",
    "\n",
    "# Convertir la string en datetime\n",
    "fete_nationale_str = \"2017_07_14\"\n",
    "### DEBUT - TO DO\n",
    "fete_nationale_date = \"A vous de jouer\"\n",
    "### FIN - TO DO\n",
    "print(fete_nationale_date)\n",
    "\n",
    "# Voyager dans le temps\n",
    "saint_sylvestre_2017_date = datetime.date(2017, 12, 31)\n",
    "bienvenu_en_2018_date = saint_sylvestre_2017_date + datetime.timedelta(days=1)\n",
    "print(bienvenu_en_2018_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperation des variables calendaires\n",
    "\n",
    "# Conversion en tant que data-frame\n",
    "xCalendaire_csv = os.path.join(data_folder, \"variablesCalendaires.csv\")\n",
    "xCalendaire_df = pd.read_csv(xCalendaire_csv, sep=\";\")\n",
    "\n",
    "# Regardons la tête des données\n",
    "xCalendaire_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recuperation des jours fériés\n",
    "jours_feries_csv = os.path.join(data_folder,\"joursFeries.csv\")\n",
    "jours_feries_df=pd.read_csv(jours_feries_csv, sep=\";\")\n",
    "\n",
    "# Pour la première colonne, les dates sont au format \"string\"\n",
    "# Nous allons les convertir en objet \"datetime\" mieux adaptés pour la manipulation de dates\n",
    "print(\"Type de la date avant transformation : \" + str(type(jours_feries_df.ds[42])))\n",
    "jours_feries_df.ds = pd.to_datetime(jours_feries_df.ds)\n",
    "print(\"Type de la date après transformation : \" + str(type(jours_feries_df.ds[42])))\n",
    "\n",
    "# Regardons la tête des données\n",
    "jours_feries_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération de la dataframe de meteo\n",
    "\n",
    "Comme d'habitude !\n",
    "\n",
    "Nos données sont dans 'meteoX_T.csv', qui est situé dans data_folder. Importez-les dans une dataframe _meteo&#95;df_\n",
    "et regardez à quoi elles ressemblent. Pensez aussi à changer les dates _string_ vers le format _datetime_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#meteo_csv = \n",
    "#meteo_df = \n",
    "#meteo_df['ds'] =\n",
    "# END\n",
    "\n",
    "meteo_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tant qu'on est chaud on enchaine avec les stations météo, fichier _StationsMeteoRTE.csv_.  \n",
    "A importer dans _stations&#95;meteo&#95;df_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# stations_meteo_csv = \n",
    "# stations_meteo_df = \n",
    "# END\n",
    "\n",
    "stations_meteo_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réduire notre problème\n",
    "On va se concentrer sur la consommation à l'échelle nationale, au pas horaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consoFrance_df = conso_df[['ds','Consommation NAT t0']]\n",
    "minutes = consoFrance_df['ds'].dt.minute\n",
    "\n",
    "indicesHour = np.where(minutes.values==0.0)\n",
    "consoFranceHoraire_df = consoFrance_df.loc[indicesHour]\n",
    "\n",
    "consoFranceHoraire_df = consoFranceHoraire_df.reset_index(drop=True)\n",
    "consoFranceHoraire_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minutes=meteo_df['ds'].dt.minute\n",
    "\n",
    "indicesHour=np.where(minutes.values==0.0)\n",
    "meteoHoraire_df=meteo_df.loc[indicesHour]\n",
    "meteoHoraire_df=meteoHoraire_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus : récupération de données depuis internet\n",
    "\n",
    "Dans le but d'automatiser un processus, nous allons faire une fonction qui ira chercher les dernières données mises à disposition sur internet.  \n",
    "Pour cet exemple nous allons considérer les jours Tempo, et (si le temps le permet en fin de TP) tester si cette information permet d'améliorer la qualité des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulation à la main\n",
    "\n",
    " - Recupérez à la main le calendrier TEMPO pour 2017-2018 :\n",
    " http://www.rte-france.com/fr/eco2mix/eco2mix-telechargement\n",
    " - Le déposer dans _data&#95;folder_\n",
    " - Le dézipper\n",
    " - Regarder les données dans excel ou autre. Notez en particulier la fin du fichier, la supprimer\n",
    " \n",
    "Importez ces données dans une dataframe avec 'read_excel' de la librairie pandas ou autre méthode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#tempo_df = \n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La même chose automatisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tempo_data(url, data_folder, tempo_xls_zip_name):\n",
    "    \n",
    "    tempo_xls_zip = os.path.join(data_folder, tempo_xls_zip_name)\n",
    "    \n",
    "    # Récupération du fichier zip depuis internet\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    http = urllib3.PoolManager()    \n",
    "    with http.request('GET', url, preload_content=False) as resp, open(tempo_xls_zip, 'wb') as out_file:\n",
    "        shutil.copyfileobj(resp, out_file)\n",
    "        \n",
    "    # On dézippe 'tempo_xls_zip' dans le repertoire 'data_folder'\n",
    "    # On pourra par exemple utliser la méthode 'extractall' de la librairie zipfile\n",
    "    # TODO\n",
    "    #\n",
    "    #\n",
    "    # END\n",
    "\n",
    "    # Petite vérification\n",
    "    if not os.path.isfile(tempo_xls_zip):\n",
    "        print(\"ERROR!! {} not found in {}\".format(\"eCO2mix_RTE_tempo_2017-2018.xls\", data_folder))\n",
    "        raise RuntimeError(\"Tempo data not uploaded :-(\")\n",
    "\n",
    "    # Import de ces données dans une dataframe\n",
    "    # (La même chose que dans la cellule ci-dessus)\n",
    "    # TODO\n",
    "    # tempo_df = \n",
    "    # END\n",
    "        \n",
    "    # Suppression du disclaimer de la dernière ligne de tempo_df, par exemple avec la méthode drop d'une dataframe\n",
    "    # TODO\n",
    "    # last_row =\n",
    "    # tempo_df = \n",
    "    # END\n",
    "\n",
    "    return tempo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test de la fonction définie ci-dessus\n",
    "\n",
    "url = \"https://eco2mix.rte-france.com/curves/downloadCalendrierTempo?season=17-18\"\n",
    "tempo_xls_zip_name = \"eCO2mix_RTE_tempo_2017-2018.zip\"\n",
    "\n",
    "tempo_df = get_tempo_data(url, data_folder, tempo_xls_zip_name)\n",
    "\n",
    "print(tempo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation des données \n",
    "\n",
    "La DataScience et le Machine Learning supposent de bien appréhender les données sur lesquelles nos modèles vont être entrainés. Pour se faire, il est utile de se faire quelques stats descriptives et des visualisations pour nos différentes variables.\n",
    "Traitant d'un problème de prévisions, on visualisera en particulier des séries temporelles.\n",
    "\n",
    "Vous allez voir:\n",
    "- échantillons de données\n",
    "- profils de courbe de consommation journaliers et saisonniers\n",
    "- visualisation de corrélation entre conso J et conso retardée\n",
    "- visualisations des stations météos\n",
    "- visualisations des séries temporelles des températures\n",
    "- calcul de corrélation sur la température entre les différentes stations météo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul de statistiques descriptives sur la consommation nationale\n",
    "A l'aide de la fonction _describe_ appliquée à la dataframe _consoFranceHoraire&#95;df_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la consommation d'un jour\n",
    "Ecrire la fonction plot_load(var_load,day,month,year) qui cherche la courbe de charge d'un jour donné et en fait la représentation graphique.\n",
    "On pourra utiliser la fonction _datetime.datetime_ pour construire une date et la fonction _datetime.timedelta(days=)_ pour incrémenter des dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load(var_load, year, month,day):\n",
    "    dtChoose = datetime.datetime(year=year, month=month, day=day)\n",
    "    dtChooseDayAfter = dtChoose + datetime.timedelta(days=1)\n",
    "    # TODO\n",
    "    #consoJour = consoFranceHoraire_df  # a continuer !\n",
    "    # END  \n",
    "    plt.plot(consoJour['ds'], consoJour[var_load],'b')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_load('Consommation NAT t0', 2016, 12, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afficher une semaine arbitraire de consommation\n",
    "On pourra modifier la fonction précédente en rajoutant le timedelta en paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_load_timedelta(var_load,year,month,day,delta_days):\n",
    "    # TODO\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # END \n",
    "      \n",
    "plot_load_timedelta('Consommation NAT t0', 2016, 12, 20, delta_days=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Calculer le profil moyen de la consommation pour les mois d'hiver et les mois d'été. Distinguer les jours de semaine des week-ends\n",
    "A l'aide de la fonction groupby et de la fonction describe.  \n",
    "Tracer le profil moyen ainsi que le min et le max pour avoir une idée de la variabilité du signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on se créé des variables calendaires\n",
    "consoFranceHoraire_df['month'] = consoFranceHoraire_df['ds'].dt.month\n",
    "consoFranceHoraire_df['weekday'] = (consoFranceHoraire_df['ds'].dt.weekday <= 5) * 1  # conversion bool => int\n",
    "consoFranceHoraire_df['hour'] = consoFranceHoraire_df['ds'].dt.hour\n",
    "consoFranceHoraire_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on fait des aggrégations\n",
    "groupedHiver = consoFranceHoraire_df[(consoFranceHoraire_df.month == 12) | \n",
    "                                         (consoFranceHoraire_df.month == 1) | \n",
    "                                         (consoFranceHoraire_df.month == 2)].groupby(['weekday','hour'], as_index=True)\n",
    "\n",
    "# TODO\n",
    "#groupedEte =  # a continuer\n",
    "#\n",
    "#\n",
    "# END\n",
    "\n",
    "statsHiver = groupedHiver['Consommation NAT t0'].aggregate([np.mean, np.min, np.max])\n",
    "\n",
    "# TODO\n",
    "#statsEte =  # a continuer\n",
    "# END\n",
    "\n",
    "statsHiver.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semaine = statsEte.loc[0]  #0 pour weekend\n",
    "\n",
    "# TODO: 3 plots semaine (mean, amin et amax), avec des couleurs\n",
    "#\n",
    "#\n",
    "#\n",
    "# END\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lien avec la consommation passée\n",
    "A l'aide de la fonction shift, calculez la consommation de l'heure précédente, du jour précédent, de la semaine précédente. Tracez ensuite la consommation en fonction de ces différentes variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consoFranceHoraire_df['lag1H'] = consoFranceHoraire_df['Consommation NAT t0'].shift(1)\n",
    "\n",
    "# TODO\n",
    "#consoFranceHoraire_df['lag1D'] =   # a continuer\n",
    "#consoFranceHoraire_df['lag1W'] =   # a continuer\n",
    "# END\n",
    "\n",
    "consoFranceHoraire_df.head(24 * 7 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def plot_scatter_load(var_x):\n",
    "    plt.scatter(consoFranceHoraire_df[var_x],consoFranceHoraire_df['Consommation NAT t0'])\n",
    "    plt.title(var_x)\n",
    "    plt.show()\n",
    "    \n",
    "plot_scatter_load('lag1H')\n",
    "plot_scatter_load('lag1D')\n",
    "plot_scatter_load('lag1W')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quelle variable semble être la plus explicative ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des stations météo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculer la corrélation de la consommation avec différentes températures (en hiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meteo_df.shape\n",
    "#Extraire la météo observée, soit les prévisions h+0\n",
    "meteo_obs_df = meteoHoraire_df[list(meteoHoraire_df.columns[meteoHoraire_df.columns.str.endswith(\"Th+0\")]) + ['ds']]\n",
    "meteo_obs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_obs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matrix_correlation = meteo_obs_df.corr().as_matrix() \n",
    "meteo_obs_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#heatMap pour un meilleur visuel\n",
    "plt.imshow(matrix_correlation,cmap='PuBu_r', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Que pensez-vous de ces corrélations ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusionner le jeu de données météo avec les données de consommation\n",
    "\n",
    "A l'aide de la fonction pd.merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "#conso_meteo = pd.merge(...)  # a continuer\n",
    "# END\n",
    "conso_meteo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verification\n",
    "conso_meteo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser la consommation en fonction de la température de la station Paris-Montsouris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(conso_meteo['156Th+0'], conso_meteo['Consommation NAT t0'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "- Qu'en pensez-vous ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des stations météo\n",
    "pour visualiser la localisation des stations météos qui nous sont fournies, positionons ces stations avec leur localisation GPS sur une Google Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstations = stations_meteo_df.shape[0]\n",
    "print(nstations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.palettes import inferno\n",
    "colors=inferno(nstations)\n",
    "print(colors)\n",
    "\n",
    "from bokeh.io import show, output_notebook\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, Range1d, PanTool, WheelZoomTool, BoxSelectTool, ColorBar, LogTicker\n",
    ")\n",
    "from bokeh.models import LabelSet, Label\n",
    "from bokeh.models import HoverTool\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "from bokeh.models.mappers import LogColorMapper\n",
    "\n",
    "map_options = GMapOptions(lat=47.08, lng=2.39, map_type=\"roadmap\", zoom=6)\n",
    "\n",
    "plot = GMapPlot(x_range=Range1d(), y_range=Range1d(), map_options=map_options)\n",
    "plot.title.text = \"France\"\n",
    "\n",
    "# For GMaps to function, Google requires you obtain and enable an API key:\n",
    "#\n",
    "#     https://developers.google.com/maps/documentation/javascript/get-api-key\n",
    "#\n",
    "# Replace the value below with your personal API key:\n",
    "plot.api_key = \"AIzaSyC05Bs_e0q6KWyVHlmy0ymHMKMknyMbCm0\"\n",
    "tempInstant1=meteo_obs_df.iloc[0,np.arange(0,35)]\n",
    "\n",
    "#nos données d'intérêt pour créer notre visualisation\n",
    "data=dict(\n",
    "        lat=stations_meteo_df['latitude'],\n",
    "        lon=stations_meteo_df['longitude'],\n",
    "        label=stations_meteo_df['Nom'],\n",
    "        temp=tempInstant1)\n",
    "\n",
    "source = ColumnDataSource(data)\n",
    "\n",
    "#l'échelle de couleur pour la température\n",
    "Tlow=0\n",
    "Thigh=20\n",
    "color_mapper = LogColorMapper(palette=\"Viridis256\",low=Tlow,high=Thigh)\n",
    "\n",
    "#la couleur de remplissage des cercles est fonction de la valeur de la temérature\n",
    "circle = Circle(x=\"lon\", y=\"lat\", size=15, fill_color={'field': 'temp', 'transform': color_mapper}, fill_alpha=0.8, line_color=None,)\n",
    "\n",
    "#les labels que l'on souhaite afficher en passant un curseur sur une station\n",
    "labels = LabelSet(x='lon', y='lat', text='label', level='glyph',x_offset=5, y_offset=5, source=source, render_mode='canvas')\n",
    "\n",
    "#on ajoute la layer\n",
    "plot.add_glyph(source, circle)\n",
    "\n",
    "#on plot\n",
    "plot.add_tools(PanTool(), WheelZoomTool(), BoxSelectTool(),HoverTool())\n",
    "\n",
    "\n",
    "color_bar = ColorBar(color_mapper=color_mapper, ticker=LogTicker(),\n",
    "                 label_standoff=12, border_line_color=None, location=(0,0))\n",
    "plot.add_layout(color_bar, 'right')\n",
    "\n",
    "\n",
    "hover=plot.select(dict(type=HoverTool))\n",
    "hover.tooltips = OrderedDict([\n",
    "    (\"index\", \"$index\"),\n",
    "    (\"(xx,yy)\", \"(@lon, @lat)\"),\n",
    "    (\"label\", \"@label\"),\n",
    "    (\"T\", \"@temp\")\n",
    "])\n",
    "\n",
    "output_notebook()#\"gmap_plot.html\"\n",
    "show(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construire un premier modèle autoregressif\n",
    "Maintenant que nous avons préparé les données et les avons analysées, nous sommes fin prêts à créer nos premiers modèles de prévision puis à les entraîner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer jeu d'entrainement et de test\n",
    "En machine learning, il y a 2 types d'erreur que l'on peut calculer : l'erreur d'entrainement et l'erreur de test. \n",
    "\n",
    "Pour évaluer la capacité de notre modèle à généraliser, il est très important de se préserver un jeu de test sur lequel nous n'aurons pas appris.\n",
    "\n",
    "Il faut donc segmenter notre dataset en 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si un a un regresseur avec du lag, il faut prendre en compte ce lag \n",
    "# et commencer l'entrainement a la date de debut des donnees +ce lag\n",
    "   \n",
    "def prepareDataSetEntrainementTest(Xinput, Yconso, dateRupture, nbJourlagRegresseur=0):\n",
    "    \n",
    "    dateStart = Xinput.iloc[0]['ds']\n",
    "    \n",
    "    DateStartWithLag = dateStart + pd.Timedelta(str(nbJourlagRegresseur)+' days')  #si un a un regresseur avec du lag, il faut prendre en compte ce lag et commencer l'entrainement a la date de debut des donnees+ce lag\n",
    "   \n",
    "    ##scindez vos datasets X et Y selon dateRupture et DateStartWithLag\n",
    "    XinputTest = Xinput[(Xinput.ds >= dateRupture)]\n",
    "    \n",
    "    # TODO\n",
    "    #XinputTrain =\n",
    "    #YconsoTrain = Yconso\n",
    "    #YconsoTest = Yconso\n",
    "    # END\n",
    "    \n",
    "    return XinputTrain, XinputTest, YconsoTrain, YconsoTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparer les variables X de notre modèle\n",
    "\n",
    "Yconso=consoFranceHoraire_df[['ds','Consommation NAT t0']]\n",
    "Yconso.columns = ['ds','y']#Consommation NAT t0' -> y car format demandé par la librairie prophet\n",
    "Xinput=Yconso #un simple modèle autoregressif, ici X=Y\n",
    "\n",
    "# on souhaite un jeu de test qui commence le 1er mai 2017\n",
    "# TODO\n",
    "#dateRupture =   # au format datetime, cf. tuto plus haut !\n",
    "# END \n",
    "\n",
    "nbJourlagRegresseur = 0  # pas de prise en compte de consommation passées pour l'instant\n",
    "\n",
    "# créer vos jeux d'entrainement et de test avec la méthode prepareDataSetEntrainementTest\n",
    "# TODO\n",
    "#XinputTrain, XinputTest, YconsoTrain, YconsoTest = \n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('la taille de l échantillon XinputTrain est:' + str(XinputTrain.shape[0]))\n",
    "print('la taille de l échantillon XinputTrain est:' + str(XinputTest.shape[0]))\n",
    "print('la taille de l échantillon XinputTrain est:' + str(YconsoTrain.shape[0]))\n",
    "print('sla taille de l échantillon XinputTrain est:' + str(YconsoTest.shape[0]))\n",
    "print('la proportion de data d entrainement est de:' + str(YconsoTrain.shape[0] / (YconsoTrain.shape[0] + YconsoTest.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créer un modèle avec Prophet\n",
    "Vous allez utiliser la librairie Prophet developpée par facebook.: https://research.fb.com/prophet-forecasting-at-scale/. Elle a été publiée en 2017 et permet de faire des modèles de prévision sur des séries temporelles. En particulier, ces modèles captent surtout des saisonnalités, et peuvent également tenir compte de jours particuliers comme les jours fériés. Il est possible de rajouter d'autre variables explicatives selon un modèle statistique linéaire.\n",
    "\n",
    "C'est une librairie relativement ergonomique et performante en terme de temps de calculs d'où son choix ici.\n",
    "Un des aspects intéressant également est qu'elle repose sur un language probabiliste PyStan. Il est ainsi possible de décrire des variables selon une loi dans notre modèle et d'obtenir sans plus de développement des intervalles de confiance et incertitudes.\n",
    "\n",
    "Pour un tutoriel bien fait pour comprendre et utiliser Prophet, je vous recommande le lien suivant: http://www.degeneratestate.org/posts/2017/Jul/24/making-a-prophet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creer un modèle prophet avec une saisonnalité journalière et une tendance nulle pour la consommation\n",
    "\n",
    "##TO DO\n",
    "#Hint: faite appel à Prophet() avec les bons parametres\n",
    "#mTrain = Prophet(...)  # on considere une tendance relativement constante pour la consommation sur les 4 ans\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainer un modèle\n",
    "Notre modèle a des paramètres tels que les saisonnalités qu'il va falloir maintenant apprendre au vu de notre jeu d'entrainement. Il faut donc caler notre modèle sur ce jeu d'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# votre modèle créé, faites un apprentissage sur vos données d'entrainement XinputTrain a partir de la méthode fit\n",
    "\n",
    "#TO DO\n",
    "#\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire des prédictions\n",
    "Une fois qu'un modèle de prévision est entrainé, il ne s'avère utile que s'il est performant sur de nouvelles situations. Faisons une prévision sur notre jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: prenez votre modèle et appelez la methode pour faire des prevision sur votre jeu de test XinputTest\n",
    "# TO DO\n",
    "#forecastTest = \n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise nos previsions avec incertitudes\n",
    "dateavantRupture = dateRupture - pd.Timedelta('30 days')  # pour visualiser aussi les réalisations d'avant\n",
    "print('on plot a partir de la date:' + str(dateavantRupture))\n",
    "mTrain.history = mTrain.history[mTrain.history.ds >= dateavantRupture]  # pour demander à Prophet de ne plotter que notre période d'interet\n",
    "mTrain.plot(forecastTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualiser le modèle\n",
    "Prophet dispose de méthodes de visualisation qui permettent d'interpreter le modèle appris, en particulier d'un point de vue des saisonalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise notre modele avec ses saisonalites \n",
    "# TODO: faites appel a une methode de prophet pour visualiser ces composantes saisonnières\n",
    "#\n",
    "# END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreter le modèle \n",
    "Au vu des visualisations précédentes, quelles interprétations pouvez-vous faire du modèle?\n",
    "Comment varie le comportement de la courbe de consommation?\n",
    "Répondez ci-dessous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluer l'erreur de prévision\n",
    "Au vu de ces previsions faites par notre modèle sur de nouvelles situations, quelle est la performance de notre modèle sur ce jeu de test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def modelError(YconsoTest,forecastTest):\n",
    "    # attention cette frame est un subset d'une autre, il faut la reindexer depuis 0\n",
    "    YconsoTest = YconsoTest.reset_index(drop=True)\n",
    "        \n",
    "    # TODO: calculer les erreurs relatives, moyenne et max avec numpy   \n",
    "    #relativeErrorsTest = \n",
    "    #errorMean = \n",
    "    #errorMax = \n",
    "    # END\n",
    "    \n",
    "    return relativeErrorsTest, errorMean, errorMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErreursTest, ErreurMoyenneTest, ErreurMaxTest = modelError(YconsoTest, forecastTest)\n",
    "print(\"L'erreur moyenne de test est de : \" + str(ErreurMoyenneTest))\n",
    "print(\"L'erreur max de test est de : \" + str(ErreurMaxTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise nos previsions par rapport a la realité\n",
    "plt.plot(YconsoTest['ds'], YconsoTest['y'], 'b')\n",
    "plt.plot(forecastTest['ds'], forecastTest['yhat'], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogramme des erreurs de prévision\n",
    "### Question\n",
    "- comment se distribue l'erreur ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 100\n",
    "plt.hist(ErreursTest,num_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "### Question\n",
    "- Quelles variables explicatives peuvent nous permettre de creer un modele plus perfomant ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle n°2: Ajout d'un régresseur sur la consommation du jour avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xinput.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creez une nouvelle variable J_1 pour Xinput avec un lag de 1 jour sur la consommation\n",
    "# TO DO\n",
    "# Xinput['J_1'] =  # a continuer\n",
    "# nbJourlagRegresseur =  # a continuer\n",
    "# END\n",
    "\n",
    "XinputTrain, XinputTest, YconsoTrain, YconsoTest = prepareDataSetEntrainementTest(Xinput, Yconso, dateRupture, nbJourlagRegresseur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shape de XinputTrain est:' + str(XinputTrain.shape[0]))\n",
    "print('shape de XinputTest est:' + str(XinputTest.shape[0]))\n",
    "print('shape de YconsoTrain est:' + str(YconsoTrain.shape[0]))\n",
    "print('shape de YconsoTest est:' + str(YconsoTest.shape[0]))\n",
    "print('la proportion de data d entrainement est de:' + str(YconsoTrain.shape[0] / (YconsoTrain.shape[0] + YconsoTest.shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer un nouveau modèle J-1' et ajouter le regresseur 'J-1' avec \"add_regressor\"\n",
    "\n",
    "#TO DO\n",
    "#mTrainConsoJ_1 = Prophet(...)\n",
    "#mTrainConsoJ_1.add_regressor()\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#entrainer ce nouveau model\n",
    "#TO DO\n",
    "#\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire des previsions\n",
    "#TO DO\n",
    "#forecastTestJ_1 = \n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErreursTestJ_1, ErreurMoyenneTest, ErreurMaxTest = modelError(YconsoTest,forecastTestJ_1)\n",
    "print(\"L'erreur moyenne de test est de : \" + str(ErreurMoyenneTest))\n",
    "print(\"L'erreur max de test est de : \" + str(ErreurMaxTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En moyenne nous avons un modèle plus performant sur notre jeu de test en passant de 5,8% d'erreur a 4,8% d'erreur.  \n",
    "En revanche l'erreur max reste significative.  \n",
    "\n",
    "### Question\n",
    "- Pourquoi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse de la sensibilité de l'erreur\n",
    "Nos variables sont essentiellement temporelles ici, essayons de voir si des instants sont particulierement mal prédits. Qu'est-ce qui en fait leur particularité ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecastTest['ds'], ErreursTestJ_1, 'r')\n",
    "plt.title(\"erreur relative sur la periode de test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberMinuteDay=60*24\n",
    "forecastTest['ErreursTest'] = ErreursTestJ_1\n",
    "forecastTest['dayInYear'] = [el.round(freq=str(numberMinuteDay)+'min') for el in forecastTest.ds]\n",
    "\n",
    "summaryByDay = forecastTest.groupby(forecastTest['dayInYear'])['ErreursTest'].max()\n",
    "print(summaryByDay.head(5))\n",
    "print(summaryByDay.describe())\n",
    "\n",
    "plt.plot(summaryByDay.index, summaryByDay, 'r')\n",
    "plt.title(\"erreur relative sur la periode de test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quels jours sont dans le quartile avec le plus d'erreur ?\n",
    "threshold = 0.15\n",
    "summaryByDay[summaryByDay >= threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de la dépendance de l'erreur selon les jours calendaires et les jours de la semaine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se rend compte que les périodes de jours feries sont mal prédites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle n°3: Ajout d'un régresseur jours feriés\n",
    "Les mois de mai et de juin 2017 étaient particulièrement chargés en jours feriés et ponts qui impactent fortement nos prédictions. Créons un modèle avec Prophet qui le modélise si on lui précise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# créez un nouveau modèle en passant à Prophet une liste de jours feriés \n",
    "# et en ajoutant toujours le régresseur conso J-1\n",
    "\n",
    "# TODO\n",
    "#mTrainConsoJ_1Holidays = Prophet(...)\n",
    "#mTrainConsoJ_1Holidays.  # ajout du regresseur\n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrainez ce nouveau modèle\n",
    "\n",
    "#TO DO\n",
    "#\n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire les prévisions\n",
    "\n",
    "#TO DO\n",
    "#forecastTestJ_1Holidays = \n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErreursTestJ_1Holidays, ErreurMoyenneTest, ErreurMaxTest = modelError(YconsoTest, forecastTestJ_1Holidays)\n",
    "print(\"L'erreur moyenne de test est de : \" + str(ErreurMoyenneTest))\n",
    "print(\"L'erreur max de test est de : \" + str(ErreurMaxTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a encore baissé l'erreur moyenne de cette periode test où les jours feriés sont nombreux, en passant de 4,8% a 4,1%. L'erreur max a elle aussi bien diminué, passant de 32% a 21%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(forecastTest['ds'], ErreursTestJ_1, 'r')\n",
    "plt.plot(forecastTestJ_1Holidays['ds'], ErreursTestJ_1Holidays, 'b')\n",
    "plt.title(\"Evolution de l'erreur en integrant les jours feriés\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bien joué ! Vous avez déjà créé un premier modèle performant pour faire des prévisions sur une fenêtre glissante à horizon 24h !\n",
    "\n",
    "Maintenant à vous de mettre votre expertise pour créer de nouveaux modèles !\n",
    "\n",
    "# Bonus: à vous de jouer\n",
    "Vous pouvez continuer à explorer le problème selon plusieurs axes:\n",
    "- créer des modèles pour les régions françaises\n",
    "- tester votre modèle sur une autre saison (lhiver par exemple)\n",
    "- créer de nouvelles variables explicatives ? Quid de la météo et de la température? du feature engineering plus complexe...\n",
    "- détecter des outliers dans les données\n",
    "- etudiez les incertitudes et les possibilités offertes par PyStan\n",
    "\n",
    "Mettez-vous en 3 groupes, explorez pendant 30 minutes, et restituez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
